\documentclass{report}

\usepackage{fullpage}
\usepackage[skip=4pt]{caption} % ``skip'' sets the spacing between the figure and the caption.
\usepackage{pgfplots}   % Needed for plotting
\usepackage{amsmath}    % Allows for piecewise functions using the ``cases'' construct
\usepackage{mathrsfs}   % Use the ``\mathscr'' command in an equation.

\usepackage[obeyspaces,spaces]{url} % Used for typesetting with the ``path'' command
\usepackage[hidelinks]{hyperref}   % Make the cross references clickable hyperlinks
\usepackage[bottom]{footmisc} % Prevents the table going below the footnote
\usepackage{nccmath}    % Needed in the workaround for the ``aligncustom'' environment
\usepackage{amssymb}    % Used for black QED symbol   

\usepackage{tabto}     % Allows to tab to certain point on a line

\newcommand{\hangindentdistance}{1cm}
\setlength{\parindent}{\hangindentdistance}
\setlength{\hangafter}{1}


% Set up page formatting
\usepackage{fancyhdr} % Used for every page footer and title.
\pagestyle{fancy}
\fancyhf{} % Clears both the header and footer
\renewcommand{\headrulewidth}{0pt} % Eliminates line at the top of the page.
\fancyfoot[LO]{CMPS242 \textendash{} Homework \#1} % Left
\fancyfoot[CO]{\thepage} % Center
\fancyfoot[RO]{Hammoudeh} %Right

% Change interline spacing.
\renewcommand{\baselinestretch}{1.1}
\newenvironment{aligncustom}
{ \csname align*\endcsname % Need to do this instead of \begin{align*} because of LaTeX bug.
    \centering
}
{
  \csname endalign*\endcsname
}
%--------------------------------------------------


\newcommand{\subproblem}[2]{~\\ (#1) \hangindent=\hangindentdistance \hangafter=2 \tabto{\hangindentdistance} \textbf{#2}~\\}


\title{\textbf{CMPS242 Homework \#2 -- Chapter \# Exercises}}
\author{Zayd Hammoudeh}

\newcommand{\problem}[3]{\noindent \textit{Chapter \##1, Problem \##2}
  \\
  \textbf{#3}  \\}

\begin{document}
  \maketitle
  
  \problem{1}{3}{Suppose that we have three colored boxes, $r$~(red), $b$~(blue), and $g$~(green).  Box~$r$ contains $3$~apples, $4$~oranges, and $3$~limes; box$~b$ contains $1$~apple, $1$~orange, and $0$~limes; box~$g$ contains $3$~apples, $3$~oranges, and $4$~limes.}
  
  \subproblem{a}{If a box is chosen at random with probabilities, $p(r)=0.2$, $p(b)=0.2$, and~$p(g)=0.6$, and a piece of fruit is removed from the box (with equal probability of select any of the items in the box), then what is the probability of selecting an apple?}
  
  Define~$B$ as the set of all boxes (i.e.,~red, blue, and green).  Then, the probability of selecting an apple can be found via:
  
  \[\Pr[apple]=\sum_{b_i \in B}\Pr[apple|b_{i}]*\Pr[b_i]\textrm{.}\]
  
  This can be rewritten as:
  
  \begin{aligncustom}
    \Pr[apple] &= \Pr[apple|r]*\Pr[r] + \Pr[apple|b]*\Pr[b] + \Pr[apple|g]*\Pr[g] \\
    \Pr[apple] &= 0.3*0.2 + 0.5*0.2 + 0.3*0.6 \\
    \Pr[apple] &= \boxed{0.34}
  \end{aligncustom}

  \subproblem{b}{If we observe that the selected fruit is in fact an orange, what is the probability that it came from the green box?}
  
  The goal of this question is to find the posterior probability,~$\Pr[g|orange]$.  The simplest way to do that is to use the priors,~$\Pr[orange]$ and~$\Pr[g]$ with the likelihood,~$\Pr[orange|g]$.  Hence:

  \begin{equation}
    \Pr[g|orange]=\frac{\Pr[orange|g]*\Pr[g]}{\Pr[orange]}\textrm{.}
    \label{eq:orangeBayes}
  \end{equation}
  
  The prior probability of orange,~$\Pr[o]$ is:
  
  \begin{aligncustom}
    \Pr[orange] &= \Pr[orange|r]*\Pr[r] + \Pr[orange|b]*\Pr[b] + \Pr[orange|g]*\Pr[g] \\
    \Pr[orange] &= 0.4*0.2 + 0.5*0.2 + 0.3*0.6 \\
    \Pr[orange] &= 0.36
  \end{aligncustom}

  This can then be substituted into the Eq.~\eqref{eq:orangeBayes}.
  
  \begin{aligncustom}
    \Pr[g|orange] &= \frac{\Pr[orange|g]*\Pr[g]}{\Pr[orange]} \\
    \Pr[g|orange] &= \frac{0.3*0.6}{0.36} \\
    \Pr[g|orange] &= \boxed{0.5}
  \end{aligncustom}



  %---------------------------------------------------%
  \newpage
  \problem{1}{9}{Show that the mode (i.e., the maximum) of the Gaussian distribution~(1.46) is given by~$\mu$.  Similarly, show that the mode of the multivariate Gaussian~(1.52) is given by~$\mathbf{mu}$.}
  
  \subproblem{a}{Show the maximum of the Gaussian distribution is given by~$\mu$.}
  
  
  
  \subproblem{b}{Show the maximum of the Gaussian distribution is given by~$\mathbf{mu}$.}


  %---------------------------------------------------%
  \newpage
  \problem{1}{11}{By setting the derivatives of the log likelihood function~(1.54) with respect to~$\mu$ and $\sigma^2$ equal to zero, verify the results~(1.55) and~(1.56).}
  
  The log likelihood function~(Equation~1.54) is defined on page~27 as:
  
  \begin{equation}
    \ln p(\textbf{x}|\mu,\sigma^2)=-\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_{n}-\mu)^2-\frac{N}{2} \ln \sigma^2 -\frac{N}{2} \ln (2\pi)
  \end{equation}
  
  \subproblem{a}{Verify Equation~(1.55) that:
      \[ \mu_{ML} = \frac{1}{N}\sum_{n=1}^{N}x_{n} \]
  }

  Take the partial derivative with respect to~$\mu$:
  
  \begin{aligncustom}
    \frac{\partial}{\partial \mu} \left( -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_{n}-\mu)^2-\frac{N}{2} \ln \sigma^2 -\frac{N}{2} \ln (2\pi) \right) &= \frac{-1}{2\sigma^2}\left( 2 * -1 * \sum_{n=1}^{N}(x_{n}-\mu) \right)  - 0 - 0 \\
    &= \frac{\sum_{n=1}^{N}(x_{n}-\mu)}{\sigma^2} \\
    &= \frac{\sum_{n=1}^{N} \left( x_{n} \right)}{\sigma^2} - \frac{N*\mu}{\sigma^2}\textrm{.}
  \end{aligncustom}

  The right side of the equation can be set equal to~$0$.  The denominator can be multiplied out resulting in:
  
  \begin{aligncustom}
    0 &= \sum_{n=1}^{N} \left( x_{n} \right) - N*\mu \\
    N*\mu &= \sum_{n=1}^{N}x_{n} \\
    \mu &= \frac{1}{N}\sum_{n=1}^{N}x_{n} ~~~\square
  \end{aligncustom}


  \subproblem{b}{Verify Equation~(1.56) that:
    \[ \sigma_{ML}^2 = \frac{1}{N}\sum_{n=1}^{N} \left( x_{n} - \mu_{ML} \right)^{2} \]
  }

  Taking the partial derivative with respect to~$\sigma$ and using the chain rule as necessary, we get:

  \begin{aligncustom}
    \frac{\partial}{\partial \sigma} \left( -\frac{1}{2\sigma^2}\sum_{n=1}^{N}(x_{n}-\mu)^{2}-\frac{N}{2} \ln \sigma^2 -\frac{N}{2} \ln (2\pi) \right) &=   -\frac{1}{2}\sum_{n=1}^{N}(x_{n}-\mu)^{2} * (-2*\sigma^{-3})  -\frac{N}{2} \left( \frac{1}{\sigma^2} (2\sigma)  \right)  - 0 \\
     &= \frac{\sum_{n=1}^{N}(x_{n}-\mu)^{2}}{\sigma^{3}} -\frac{N}{\sigma}
  \end{aligncustom}

  This equation is set equal to~$0$ and everything multiplied by~$\sigma$ resulting in:
  
  \begin{aligncustom}
    0 &= \frac{\sum_{n=1}^{N}(x_{n}-\mu)^{2}}{\sigma^{2}} - N \\
    N &= \frac{\sum_{n=1}^{N}(x_{n}-\mu)^{2}}{\sigma^{2}} \\
    \sigma^{2} &= \frac{\sum_{n=1}^{N}(x_{n}-\mu)^{2}}{N} \textrm{.}
  \end{aligncustom}

  The optimal value for~$\mu$ was found in part~(a) which can then substituted:
  
  \begin{aligncustom}
    \sigma^{2} &= \frac{\sum_{n=1}^{N}(x_{n}-\mu_{ML})^{2}}{N} \textrm{.} ~~~\square
  \end{aligncustom}


  %---------------------------------------------------%
  \newpage
  \problem{1}{29}{Consider an $M$-state discrete random variable,~$x$, and use Jensen's inequality in the form~(1.115) to show that the entropy of its distribution~$p(x)$ satisfies~$\textrm{H}[x] \leq \ln M$.}
  
  Equation~(1.115) in the textbook (page~56) states:
  
  \begin{equation}
    f \left( \sum_{i=1}^M{\lambda_{i} f(x_i)} \right) =\sum_{i=1}^M{\lambda_{i} f(x_i)}
    \label{eq:q9eq115}
  \end{equation}


  %---------------------------------------------------%
  \newpage
  \problem{1}{30}{Evaluate the Kullback-Leiber divergence~(1.113) between two Gaussians,~$p(x)=\mathscr{N}(x|\mu,\sigma^2)$ and~$q(x)=\mathscr{N}(x|m,s^2)$.}


  %---------------------------------------------------%
  \newpage
  \problem{1}{31}{Consider two variables~$\mathbf{x}$ and~$\mathbf{y}$ having joint distribution~$p(\mathbf{x},\mathbf{y})$, show that the differential entropy of this pair of variables satisfies
  \[\textrm{H}[\mathbf{x},\mathbf{y}] \leq \textrm{H}[\mathbf{x}] + \textrm{H}[\mathbf{y}]\]
  with equality if and only if,~$\mathbf{x}$ and~$\mathbf{y}$ are statistically independent.}


  %---------------------------------------------------%
  \newpage
  \problem{1}{40}{By applying Jensen's inequality~(1.115) with~$f(x)=\ln x$, show that the arithmetic mean of a set of real numbers is never less than their geometrical mean.}
  
  For a set of numbers,~$\{x_1,...,x_M\}$, the arithmetic mean is defined as:
  
  \[ \frac{\sum_{i=1}^{M}x_i}{M} \]
  
  In contrast, for the same set of numbers, the geometric mean is defined as:
  
  \[ \left( \prod_{i=1}^{M}x_i \right)^\frac{1}{M}\textrm{.} \]
  
  Jensen's Inequality~(1.115) is defined as:
  
  \begin{equation}
    f \left( \sum_{i=1}^M{\lambda_{i} f(x_i)} \right) \leq \sum_{i=1}^M{\lambda_{i} f(x_i)}\textrm{.}
    \label{e40:q9eq115}
  \end{equation}
  
  If we substitute for~$f(x)=\ln x$, this becomes:

  \[ ln \left( \sum_{i=1}^M{\lambda_{i} \ln(x_i)} \right) \leq \sum_{i=1}^M{\lambda_{i} \ln(x_i)}\textrm{.} \]
  
  For Jensen's Inequality to hold, $\lambda$~must satisfy two conditions namely:~$\lambda_i \geq 0$ and~$\sum_{i=1}^{M}\left( \lambda_i \right) = 1$.  An obvious satisfying case is~$\lambda_i=1/M$ for all~$i$.  This then changes the equation to:
  
  \[ ln \left( \frac{\sum_{i=1}^{M}{\ln(x_i)}}{M} \right) \leq \frac{1}{M}\sum_{i=1}^M{\ln(x_i)}\textrm{.} \]
  
  Using the properties of logarithms, the right side is transformable to a product via:
  
  \[ ln \left( \frac{\sum_{i=1}^{M}{\ln(x_i)}}{M} \right) \leq \frac{1}{M}{\ln \left( \prod_{i=1}^M{x_i} \right)}\textrm{.} \]
  
  Using another property of logarithms, the fraction on the right side can be brought inside the logarithm as:
  
  \[ ln \left( \frac{\sum_{i=1}^{M}{\ln(x_i)}}{M} \right) \leq \ln \left( \left( \prod_{i=1}^M{x_i}\right)^\frac{1}{M} \right) \textrm{.} \]
  
  Both sides are then raised to the power of~$e$ completing the proof that it is never less than.
  
    \[ \frac{\sum_{i=1}^{M}{\ln(x_i)}}{M} \leq \left( \prod_{i=1}^M{x_i}\right)^\frac{1}{M}  ~~~\square \]

\end{document}