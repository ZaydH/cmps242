\documentclass{report}

\usepackage{fullpage}
\usepackage[skip=4pt]{caption} % ``skip'' sets the spacing between the figure and the caption.
\usepackage{pgfplots}   % Needed for plotting
\usepackage{amsmath}    % Allows for piecewise functions using the ``cases'' construct
\usepackage{graphics}   % Allows figures in the document.
\graphicspath{{img/}}
\usepackage{multicol}   % Used for two column lists

\usepackage{tikz}
\usetikzlibrary{matrix, positioning, calc}
\usepackage{xcolor}

%\usepackage[hidelinks]{hyperref}   % Make the cross references clickable hyperlinks
%\usepackage[bottom]{footmisc} % Prevents the table going below the footnote

% Set up page formatting
\usepackage{fancyhdr} % Used for every page footer and title.
\pagestyle{fancy}
\fancyhf{} % Clears both the header and footer
\renewcommand{\headrulewidth}{0pt} % Eliminates line at the top of the page.
\fancyfoot[LO]{CMPS242 \textendash{} Homework \#5} % Left
\fancyfoot[CO]{\thepage} % Center
\fancyfoot[RO]{Sherman \& Hammoudeh} %Right

\renewcommand\thesection{\arabic{section}} % Prevent chapter number in section number.

% Change interline spacing.
\renewcommand{\baselinestretch}{1.1}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}


\title{\textbf{CMPS242 Homework \#5 \textendash{} \\Neural Network Tweet Classification}}
\author{Benjamin Sherman \\~\\ \& \\~\\ Zayd Hammoudeh \\~\\ \textbf{Team Name}: ``Who Farted?''}
\date{} % Remove date on cover page


\begin{document}
  \maketitle
  
  \suppressfloats % No images on the first page.
  \section{Homework Objective}
  
  Develop a long short term memory-based (LSTM) neural network that classifies tweets as being sent by either Hillary Clinton (@HillaryClinton) or Donald Trump (@realDonald Trump).
  
  \section{Dataset Overview}
  
  The dataset is a comma-separated list of tweets from either @HillaryClinton or @realDonaldTrump.  No additional information is provided by the tweet's text and potentially the class label.  The training set consists of 4,743~total tweets while the test set had 1,701~unlabeled tweets.
   
  \section{Preprocessor}
  
  The dataset consists of tweet 
  
  \section{Classifier Structure}
  
  The LSTM classifier for this homework is required to at minimum the structure shown in Figure~\ref{fig:completeLstmClassifer}.  There are four primary components, namely the embedding matrix, one-hot vector, long short-term memory network, and feed-forward network.  These modules are described in the following subsections.
  
  \begin{figure}
    \centering
    \tikzset{
      sigmoid/.style={path picture= {
          \begin{scope}[x=1pt,y=10pt]
            \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
          \end{scope}
        }
      },
      every neuron/.style={
        circle,
        draw,
        minimum size=1cm,
      },
      neuron output/.style={
        every neuron,
        sigmoid
      },
      matrix multiply/.style={
        every neuron,
        font={\Large $\times$}
      },
      every netbox/.style={
        rectangle,
        draw,
        text centered,
        text width = 1.5cm,
        minimum width = 1.75cm,
        minimum height = 2cm
      },
      feed forward/.style={
        every netbox,
        font={Feed Forward Neural Network},
      },
      lstm/.style={
        every netbox,
        font={LSTM},
      },
      every left delimiter/.style={xshift=1ex},
      every right delimiter/.style={xshift=-1ex},
      bmatrix/.style={matrix of math nodes,
        inner sep=0pt,
        left delimiter={[},
        right delimiter={]},
        nodes={anchor=center, inner sep=.3333em},
      }
    }
    
    \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth] 
    
      % Must define nodes in the list before they can be used.
      \node [neuron output] (output) {};
      \node [feed forward, left of=output, xshift=-0.7in] (feedforward) {};
      \node [lstm, left of=feedforward, xshift=-0.8in] (lstmbox) {};
      \node [matrix multiply, left of=lstmbox, xshift=-0.5in] (matmul) {};
      \matrix (onehot) [bmatrix, below left of = matmul, xshift=-0.5in, yshift = -0.5in] {
        0 \\
        1 \\
        0 \\[-1ex]
        \vdots \\
        0\\};
      \node [left of=onehot, xshift=0.1in] (onehotcontinue) {\Large $\cdots$};
      \node [left of=onehotcontinue, text width = 0.82in, text centered, xshift =-0.2in] (onehotlabel) {One-Hot Vector Input};
      \matrix (embedding) [bmatrix, above left of = matmul, xshift=-1in, yshift=0.2in] {
        w_{1,1} & w_{1,2} &\cdots & w_{1,N}\\
        w_{2,1} & w_{2,2} &\cdots & w_{2,N}\\[-1ex]
        \vdots & & &  \vdots\\
        w_{d,1} & w_{d,2} &\cdots & w_{d,N}\\};
      \node [above of=embedding, yshift=0.15in] (embeddinglabel) {Embedding Matrix};
                  
      % Draw arrows
      \draw [->] (embedding) -- (matmul);
      \draw [->] (onehot) -- (matmul);
      \draw [->] (matmul) -- (lstmbox);
      \draw [->] (lstmbox) -- (feedforward);
      \draw [->] (lstmbox) to[out=30, in=150, loop, looseness=4] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
      \draw [->] (feedforward) -- (output);
      \draw [->] (output) -- ++(1,0);
    \end{tikzpicture} 
    \caption{Base LSTM Neural Network Classifier}\label{fig:completeLstmClassifer}
  \end{figure}
  
  \subsection{One-Hot Vector Input} \label{sec:oneHotVector}
  
  \subsection{Embedding Matrix} \label{sec:embeddingMatrix}
  
  \subsection{Long Short-Term Memory Network Structure} \label{sec:lstemOverview}
  
  
  \subsection{Feed-Forward Neural Network Structure} \label{sec:neuralNetOverview}
  
  \begin{figure}
    \centering
    \tikzset{
      sigmoid/.style={path picture= {
          \begin{scope}[x=1pt,y=10pt]
            \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
          \end{scope}
        }
      },
      relu/.style={path picture= {
          \begin{scope}[x=1pt,y=1pt]
            \draw plot[thick,domain=-8:0] (\x,{-3});
            \draw plot[ultra thin,domain=0:8] (\x,{-3+\x});
          \end{scope}
        }
      },
      every neuron/.style={
        circle,
        draw,
        minimum size=1cm,
      },
      neuron filled/.style={
        every neuron,
        fill=gray!10
      },
      neuron bias/.style={
        every neuron,
        font={Bias}
      },
      neuron output/.style={
        every neuron,
        sigmoid
      },
      neuron hidden/.style={
        every neuron,
        relu
      },
      neuron missing/.style={
        draw=none, 
        scale=4,
        text height=0.3cm,
        execute at begin node=\color{black}$\vdots$
      },
    }
  
    
    \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
      \foreach \m/\l [count=\y] in {1,missing,2}
        \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,1.1-\y*0.9) {};
      
      %\foreach \m [count=\y] in {1,2,missing,3}
      %  \node [neuron hidden/.try, neuron \m/.try ] (hidden-\m) at (2,2.5-\y*1.25) {};
      \node [neuron hidden/.try, neuron 1/.try ] (hidden-1) at (2,2.5-1*1.25) {};
      \node [neuron hidden/.try, neuron 2/.try ] (hidden-2) at (2,2.5-2*1.25) {};
      \node [every neuron/.try, neuron missing/.try ] (hidden-missing) at (2,2.5-2.95*1.25) {};
      \node [neuron hidden/.try, neuron 3/.try ] (hidden-3) at (2,2.5-4*1.25) {};
      
      \foreach \m [count=\y] in {1,2}
        \node [neuron output/.try, neuron \m/.try ] (output-\m) at (4,0.85-\y*1.1) {};
      
      \foreach \m [count=\y] in {1,2}{
        \node [neuron filled/.try] (filled-\m) at (-2+2*\y,-3.5) {};
        \node [neuron bias/.try] (bias-\m) at (-2+2*\y,-3.5) {};
      }
      
      % Add the labels to the nodes
      \foreach \l [count=\i] in {1,d}
        \draw [<-] (input-\i) -- ++(-1,0)
        node [above, midway] {$I_\l$};
      
      \foreach \l [count=\i] in {1,2,m}
        \node [above] at (hidden-\i.north) {$H_\l$};
      
      \foreach \l [count=\i] in {1,2}
        \draw [->] (output-\i) -- ++(1,0)
        node [above, midway] {$O_\l$};
      
      % Draw the connecting arrows
      \foreach \i in {1,...,2}
        \foreach \j in {1,...,3}
          \draw [->] (input-\i) -- (hidden-\j);
      
      \foreach \i in {1,...,3}
        \foreach \j in {1,...,2}
          \draw [->] (hidden-\i) -- (output-\j);

      \foreach \i in {1,...,3}
        \draw [loosely dashed, ->] (bias-1) -- (hidden-\i);
      \foreach \i in {1,...,2}
        \draw [loosely dashed, ->] (bias-2) -- (output-\i);
          
      
      \foreach \l [count=\x from 0] in {Input, Hidden, Output}
      \node [align=center, above] at (\x*2,2) {\l \\ Layer};
    \end{tikzpicture} 
    \caption{Base Structure of Our Feed-Forward Network}\label{fig:feedForwardNet}
  \end{figure}
  
  The final structure of our feed forward network is shown in Figure~\ref{fig:feedForwardNet}.  The number of input nodes is dictated by the number of rows in the embedding matrix; as mentioned in Section~\ref{sec:embeddingMatrix}, we used a rank of~$d=25$ in our experiments.  Our sole hidden layer has $m=256$ fully-connected neurons. There are two output nodes (e.g., one for ``Donald Trump'' and the other for ``Hillary Clinton).  A softmax function normalizes the output probabilities to ensure they sum to probability~$1$.  We selected this paradigm as it simplified the Tensor Flow implementation without affecting the quality of the results.
  
  Our final feed-forward network design used the rectified linear and sigmoid activation functions for the hidden and output layers respectively.  Each neuron in the network also had its own bias input.
  
  \subsection{Implementation} \label{sec:implementation}
  
  As specified in the homework description, our network is written in Python (specifically version~3.5.3) using Google's TensorFlow library.  Additional packages we used include: Pandas, NumPy, Scikit-Learn (for text preprocessing and vectorizing), and mlxtend (``Machine Learning Library Extension'' for generating the one-hot). 
  
  \section{Experimental Results} \label{sec:experimentalResults}
  
  \section{Extra Credit \#1: Bag of Words Model}
  
  In the ``bag of words'' model, each textual input, i.e., tweet, is transformed into an unordered set of words.  Hence, the contextual and word ordering information is discarded.  This approach removes any sequential relation in the data; hence, the LSTM added no specific value for training.  Hence, we removed the LSTM when performing this experiment and instead trained with just the embedding matrix and the feed-forward network. 
  
  Using the previously described neural-network structure, we were able to get 100\%~accuracy on the complete training set.  Likewise, we get an best-case test set of~\textbf{0.20070} using this approach.
  
  \section{Extra Credit \#2: Neural Network Experiments}
  
  Below are additional experiments we tried beyond the base requirements.  
  
  \subsection{Extra Credit \#2a: Hidden Layer Activation Functions}
  
  We experimented with three activation configurations for the hidden layer.  In addition to rectified linear, we also tried a ``pass-through'' activation where the neuron's output was simply~$\textbf{w}\cdot\textbf{x} + b$, where $\textbf{w}$~is the weight vector, $\textbf{x}$~the input, and $b$~the bias term.  We also tried to use a sigmoid function.  However, these two other activation functions took more epochs to converge (an increase of approximately 200 to 1,000~training rounds) and yield worse testing error.
  
  \subsection{Extra Credit \#2b: Additional Feed-Forward Hidden Layers}
  
  
  \subsection{Extra Credit \#2b: Additional Neurons in the Hidden Layer}
  
  This is a general (but not strict) correlation between the number of neurons in the hidden layer and the complexity of the function the network can learn.  Additional neurons also increase the possibility of overfitting.  We experiment with three different quantities of hidden layer neurons, namely $128$, $256$, and $512$.  We observed that $128$ and $512$ neurons had similarly poor performance of approximately~$\textbf{0.24}$ on the test set.  In addition, $512$ neurons significantly increased the training time (by a factor of two times).  In contrast, $256$~hidden layer neurons had a training error of~$\sim0.20$, that is why we selected~$d=256$ as discussed in Section~\ref{sec:neuralNetOverview}.
     
\end{document}