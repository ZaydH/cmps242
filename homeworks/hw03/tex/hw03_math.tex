\documentclass{report}

\usepackage{fullpage}
\usepackage[skip=4pt]{caption} % ``skip'' sets the spacing between the figure and the caption.
\usepackage{pgfplots}   % Needed for plotting
\usepackage{amsmath}    % Allows for piecewise functions using the ``cases'' construct
\usepackage{grffile}   % Allows period in the graphics file name

\usepackage[obeyspaces,spaces]{url} % Used for typesetting with the ``path'' command
\usepackage[hidelinks]{hyperref}   % Make the cross references clickable hyperlinks
\usepackage[bottom]{footmisc} % Prevents the table going below the footnote
\usepackage{color}


% Set up page formatting
\usepackage{fancyhdr} % Used for every page footer and title.
\pagestyle{fancy}
\fancyhf{} % Clears both the header and footer
\renewcommand{\headrulewidth}{0pt} % Eliminates line at the top of the page.
\fancyfoot[LO]{CMPS242 \textendash{} Homework \#3 Math Proposal} % Left
\fancyfoot[CO]{\thepage} % Center
\fancyfoot[RO]{Zayd Hammoudeh} %Right

\renewcommand\thesection{\arabic{section}} % Prevent chapter number in section number.

\newcommand{\eref}[1]{(\ref{#1})}
\newcommand{\yhat}{\hat{y}}
\newcommand{\wstar}{\mathbf{w}^{\star}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\includeLambdaPlot}[3]{  
\begin{subfigure}[t]{#2}
  \centering
  \includegraphics[width=1.0\textwidth]{test_train_compare_d=9_lambda=#1.pdf}
  \caption{$\lambda=#1$}\label{#3}
\end{subfigure}}

% Change interline spacing.
\renewcommand{\baselinestretch}{1.1}

\newcommand{\T}{^\textrm{T}}
\title{\textbf{CMPS242 Homework \#3 \textendash{} Mathematical Foundations of Logistic Regression}}
\author{Zayd Hammoudeh}

\begin{document}
  %\maketitle

  \section{Source}
  
  To the best of my recollection, the details of the logistic regression update rule math were not reviewed in class.  As such, I watched√ü the lectures from \href{https://www.coursera.org/learn/neural-networks-deep-learning/home/week/2}{\color{purple} Andrew Ng's deep learning class}.  Here is what I \textit{believe} the update rule should be.  If there is an error in the logic, please let me know.
  
  \section{Glossary of Notation}
  
  \begin{itemize}
    \item $\mathbf{w}_{t}$ -- Weight vector for epoch~$t$
    \item $J(\mathbf{w},\mathbf{x})$ -- Cost function
    \item $\beta$ -- Learning rate
    \item $\mathcal{L}$ -- Loss function
    \item $\yhat$ -- Predicted output value
    \item $y$ -- Expected (target) classification value
    \item $\sigma(z)$ -- Sigmoid function ${\left(\frac{1}{1+e^{-z}}\right)}$ with respect to~$z$ (i.e.,~${\mathbf{w}^{\T} \mathbf{x}}$).
  \end{itemize}
  
  \section{$w$~Update Rules with Squared Loss}
  
  My understanding of the \textit{batch} update rule is shown in Eq.~\eqref{eq:batchUpdateRule}.
  
  \begin{equation}
    \mathbf{w}_{t+1} := \mathbf{w}_{t} - \beta \cdot \frac{\partial J(\mathbf{w}, \mathbf{x})}{\partial \mathbf{w}}
    \label{eq:batchUpdateRule}
  \end{equation}
  
  \noindent
  The learning rate is defined as:
  
  \[\beta := \eta \cdot t^{-\alpha} \]
  
  \noindent
  where~$\alpha=0.9$.  The cost function is the average loss as shown in Eq.~\eqref{eq:costFunction}.
 
  \begin{equation}
    J(\mathbf{w}, \mathbf{x}) = \frac{1}{m}\sum_{i=1}^{m} \mathcal{L}(\mathbf{w},\mathbf{x})
    \label{eq:costFunction}
  \end{equation}
  
  \noindent
  The loss function is the squared loss and uses the same regularizer as in homework~\#1 as shown in Eq.~\eqref{eq:squaredLossFunction}.
  
  \begin{equation}
    \mathcal{L}(\yhat,y,\mathbf{w}) = \frac{1}{2}(\yhat - y)^2 + \lambda  \norm{\mathbf{w}}
    \label{eq:squaredLossFunction}
  \end{equation}
  
  \noindent
  The predicted value~$\yhat$ is 
  
  \[\yhat = \sigma   (\mathbf{w}^{\T}\mathbf{x}) \textrm{.} \]

  The derivative of the loss function~$\mathcal{L}$ is:
  
  \begin{align}
    \frac{\partial \mathcal{L}(\mathbf{w}, \mathbf{x})}{\partial \mathbf{w}} = (\yhat - y)  \frac{\partial \yhat}{\partial \mathbf{w}} + \lambda \mathbf{w} \textrm{.}
    \label{eq:derivLoss}
  \end{align}

  \noindent
  Via the chain rule, we solve the derivative of the sigmoid function:
  
  \begin{equation}
    \frac{\partial \yhat(z)}{\partial \mathbf{w}} = \sigma(z) (1-\sigma(z)) = \frac{e^{-z}}{(1+e^{-z})^{2}}  \frac{\partial z}{\partial \mathbf{w}}
    \label{eq:derivYhat}
  \end{equation}

  \noindent
  Applying the chain rule again yields:
  
  \begin{equation}
    \frac{\partial z}{\partial \mathbf{w}} = \mathbf{x}
    \label{eq:derivZ}
  \end{equation}

  \noindent
  Combining Eq.~\eqref{eq:derivLoss},~\eqref{eq:derivYhat}, and~\eqref{eq:derivZ} shows the complete derivative of the loss function.
  
  \begin{align}
    \frac{\partial \mathcal{L}(\mathbf{w}, \mathbf{x})}{\partial \mathbf{w}} &= \left( \sigma(\mathbf{w}^{\T} \mathbf{x}) - y \right) \left( \frac{e^{-z}}{(1+e^{-z})^{2}} \right) \mathbf{x} + \lambda \mathbf{w}\\
    &= \left( \sigma(\mathbf{w}^{\T} \mathbf{x}) - y \right) \left( \frac{e^{-\mathbf{w}^{\T} \mathbf{x}}}{(1+e^{-\mathbf{w}^{\T} \mathbf{x}})^{2}} \right) \mathbf{x} + \lambda \mathbf{w}
    \label{eq:completeDerivative}
  \end{align}  
  
  \noindent
  For mathematical simplicity, the identity for~$\sigma'(z)$ allows for a simpler form in Eq.~\eqref{eq:completeDerivativeSimple}.\footnote{I am not considering the transposes.  That math I would need to think more about.}
  
  \begin{equation}
    \frac{\partial \mathcal{L}(\mathbf{w}, \mathbf{x})}{\partial \mathbf{w}} = \left( \sigma(\mathbf{w}^{\T} \mathbf{x}) - y \right) \sigma(\mathbf{w}^{\T} \mathbf{x}) \left( 1 - \sigma(\mathbf{w}^{\T} \mathbf{x}) \right)  \mathbf{x} + \lambda \mathbf{w}
  \label{eq:completeDerivativeSimple}
  \end{equation} 
    
  \section{$w$~Update Rules with Logistic Loss}
  
  The more common loss function I see for logistic regression is in Eq.~\eqref{eq:logisticLossFunction}.
  
  \begin{equation}
    \mathcal{L}(\yhat,y,\mathbf{w}) = -\left(  y\log(\yhat) + (1-y) \log(1-\yhat) \right)
    \label{eq:logisticLossFunction}
  \end{equation}
  
  \noindent
  The derivative of the logistic loss function is shown below in Eq.~\eqref{eq:derivLogisticLoss}.
  
  \begin{equation}
    \frac{\partial \mathcal{L}(\yhat,y,\mathbf{w})}{\partial \mathbf{w}} = -\left(  \frac{y}{\yhat} - \frac{1-y}{1-\yhat} \right) \frac{\partial \yhat}{\partial \mathbf{w}}
    \label{eq:derivLogisticLoss}
  \end{equation}
  
  \noindent
  We know the derivative of~$\yhat$ from Eq.~\eqref{eq:derivYhat}.  Substituting that we get:
  
  \begin{align}
    \frac{\partial \mathcal{L}(\yhat,y,\mathbf{w})}{\partial \mathbf{w}} &= -\left(  \frac{y}{\yhat} - \frac{1-y}{1-\yhat} \right) \yhat(1-\yhat) \frac{\partial \mathbf{z}}{\partial \mathbf{w}}\\
    &= \left( -y(1 - \yhat) + (1-y) \yhat) \right) \frac{\partial \mathbf{z}}{\partial \mathbf{w}}\\
    &= \left( \yhat - y \right) \frac{\partial \mathbf{z}}{\partial \mathbf{w}} \textrm{.}
    \label{eq:derivLogistic}
  \end{align}

  \noindent
  The complete derivative then is in Eq.~\eqref{eq:derivLogisticComplete}.
  
  \begin{equation}
    \frac{\partial \mathcal{L}(\yhat,y,\mathbf{w})}{\partial \mathbf{w}} = \left( \yhat - y \right) \mathbf{x} \textrm{.}
    \label{eq:derivLogisticComplete}
  \end{equation}
  

\end{document}