\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tikz}
\usetikzlibrary{matrix, positioning, calc, shadows, decorations.markings, arrows.meta} % decorations.markings is used for the bus symbol on the arrow.


\title{Make Deep Learning Great Again: Character-Level RNN Speech Generation in the Style of Donald Trump}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Benjamin Sherman \\
  Department of Computer Science\\
  University of California, Santa Cruz\\
  Santa Cruz, CA 95064 \\
  \texttt{bcsherma@ucsc.edu} \\
  \And
  Zayd Hammoudeh \\
  Department of Computer Science\\
  University of California, Santa Cruz\\
  Santa Cruz, CA 95064 \\
  \texttt{zayd@ucsc.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  A character-level recurrent neural network (RNN) is a statistical language model capable of producing text that superficially resembles a training corpus. The efficacy of these networks in mimicking Shakespeare, \texttt{Linux} source code, and other forms of text have already been demonstrated. In this paper, we show that character level RNNs are capable of very believably mimicking the language of President Donald J. Trump after training on a corpus of speech transcriptions. We believe our most significant contributions to the study of character level statistical language models are in sampling methodologies; specifically we propose that introducing dropout during the text-generation phase introduces randomness that leads to more believable text.
\end{abstract}

% introduction
\section{Introduction}


% character level RNN overview
\section{Character Level RNNs}

A character level RNN is a statistical model of language. It is statistical in the sense that it views the behavior of language probabilistically. To better understand this, allow that my brain uses a statistical language model and that my job is to try to finish all of your sentences. You say ``I am going to the grocery-''. I would guess with high probability that you are about to say ``store'', though I have to accept that there is a non-zero chance you will say ``outlet''- or, perhaps you will stub your toe and say ``ouch!'' in the middle of your sentence. What a character level RNN does is really no different, except the inferences it learns to make happen on the character level.

Let $\mathcal{C} : V^L \rightarrow P$ be a character-level recurrent neural network, such that $V$ is a vocabulary of approximately 100 characters, $L$ is an arbitrary sequence length, and $P$ is the set of all probability distributions over $V$. More plainly, we can think of a character-level RNN as a function that takes a sequence of $L$~characters and gives us a probability distribution; in particular, it gives the probability of the next character given the previous $L$ characters. As an example of how the network will ideally behave, imagine that you let $p = \mathcal{C}(\texttt{(M,a,k,e, ,A,m,e,r,i,c)})$. If the network is well trained you should find that $p$ predicts \texttt{a} as the next character with high probability.

In order to generate meaningful text with a character level RNN you need to give it a seed, similar to the sentence-to-be-finished from the game described earlier. Prompted with the sequence ``We will build a g'', the RNN will produce a distribution over the vocabulary. We can sample from this distribution and add the resulting character to the sequence. If the sampled character was `r`, as we would hope, then we now have the sequence ``We will build a gr''. We can continue this process for arbitrarily many steps. It is important to note that we can not make networks that accept arbitrarily long sequences as input, so at some point we have to start removing a character from the beginning of the sequence for every character we add to the end of the sequence.

% our training process
\section{Training}

\subsection{Vocabulary}

% dataset overview
\subsection{Dataset}

% add some statistics and an overview of where we got the data etc.
Character-level RNNs can be provided any user-supplied text as a seed.  The current Oxford English dictionary has more than 170,000 words.  While some words are substantially more common than others, it does not change the fact that a character-level RNN must be able to generate meaningful outputs from countless many input seeds. Therefore, to achieve acceptable performance, the training set needs to be especially large.

Although President Trump is credited with being the lead author of over a dozen books~\cite{trumpArtOfTheDeal,trumpSurvivingAtTheTop,trumpArtOfTheComeback,trumpTheAmericaWeDeserve,trumpHowToGetRich,trumpCrippledAmerica,trumpTimeToGetTough,trumpThinkBig,trump101,trumpNeverGiveUp,trumpWhyWeWantYou,trumpBestGolfAdvice,trumpMidasTouch}, we also deliberately chose to exclude them from the training set also for two primary reasons.  First, many of the books featured co-authors or were entirely ghostwritten~\cite{mayerNewYorkerGhostwriter}.  As such, it would be difficult to distinguish Trump's own style from those of his writing partners.  In addition, most of Trump's books date back to the late 1980's through the early 2000's.  Most of the students in the course had not even been born by the time some of these books were written. Hence, the generated text they may yield may not be meaningful to the class's relatively young audience.

Another possible source of training content are Trump's tweets, but we did not use them in this project for multiple reasons.  First, Twitter limits tweets to only 144~characters. As such, tweeters deliberately prune content and commentary to fit within this strict type limit.  This leads to extensive use of abbreviations, skipping of words, or hastags (e.g., ``\#MAGA'' for ``Make America Great Again'') many of which are exclusive to the Twitter platform. For example, one of President Trump's signature Twitter mannerisms is to end a tweet with ``Sad!''; however, this is not done in everyday speech even by the president himself. In addition, similar to at least some of Mr. Trump's books, many of his tweets are ghostwritten.  It has been reported that at least White House social media director Dan Scavino Jr.~\cite{ohlheiser2017} and Trump lawyer, John Dowd~\cite{phillipsBlake2017}, have authored tweets in Trump's name.  These ``imposter'' tweets risk polluting the data set with non-Trump content.

Given the deficiencies associated with training on tweets or the president's book, we decided to exclusively limit ourselves to public speeches made by Mr. Trump since he announced his bid to run for president on June 16, 2015.  Some of these public speeches had already been compiled in repository in~\cite{ryanMcdermottTrumpSpeeches}.  Another repository of largely different speeches had been collected in a separate repository~\cite{pedramNavidTrumpSpeeches}.  Unlike the first set of speeches which was a static collection in text format, the second set used a web scraper to pull tweets from the University of California, Santa Barbara's campaign speech archive~\cite{americanPresidencyProject}.  However, the web scraper had significant bugs that was corrupting the speech output.  We modified the script using Python's \texttt{BeautifulSoup4} package to properly extract the tweets.  We then manually merged the two training sets verifying there were no duplicates.

In total, the training set consisted of more than 115 speeches of varying length.  There were more than 365,000~words and two million training sequences.  We believe the training set size is more than sufficient for a project of this scope, and we are further confident this is shown in the quality of the generated output.


% architecture
\subsection{Learner Architecture}

\begin{figure}
  \centering
  \tikzset{
    sigmoid/.style={path picture= {
        \begin{scope}[x=1pt,y=10pt]
          \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
        \end{scope}
      }
    },
    every neuron/.style={
      circle,
      draw,
      fill=white,
      minimum size=0.75cm,
    },
    neuron output/.style={
      every neuron,
      sigmoid,
    },
    matrix multiply/.style={
      every neuron,
      font={\Large $\times$}
    },
    every netbox/.style={
      rectangle,
      draw,
      text centered,
      fill=white,
      text width = 1.3cm,
      minimum width = 1.5cm,
      minimum height = 1.5cm,
      rounded corners,
    },
    every left delimiter/.style={xshift=1ex},
    every right delimiter/.style={xshift=-1ex},
    bmatrix/.style={matrix of math nodes,
      inner sep=0pt,
      left delimiter={[},
      right delimiter={]},
      nodes={anchor=center, inner sep=.3333em},
    },
    decoration={
      markings,
      mark= at position 0.5 with {\node {/};}
    }
  }
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth] 
  % Must define nodes in the list before they can be used.
  \node [neuron output, drop shadow] (softmax) {};
  \node [every netbox, left of=softmax, xshift=-0.35in, drop shadow] (feedforward) {\small Feed Forward Network};
  \node [every netbox, left of=feedforward, xshift=-0.45in, drop shadow] (lstmbox) {\small LSTM};
  \node [every neuron, left of=lstmbox, xshift=-0.25in, drop shadow] (matmul) {\Large$\times$};
  \matrix (oneHotFirst) [bmatrix, below left of = matmul, xshift=-0.2in, yshift = -0.5in] {\tiny
    0 \\
    1 \\
    0 \\[-1.5ex]
    \vdots \\
    0\\};
  \matrix (oneHotSecond) [bmatrix, left of = oneHotFirst, xshift=0.1in] {\tiny
    0 \\
    1 \\
    0 \\[-1.5ex]
    \vdots \\
    0\\};
  \node [left of=oneHotSecond, xshift=0.1in] (oneHotContinue) {\Large $\cdots$};
  \matrix (oneHotLast) [bmatrix,left of = oneHotContinue, xshift=0.05in] {\tiny
    1 \\
    0 \\
    0 \\[-1.5ex]
    \vdots \\
    0\\};
  \node [above of=oneHotContinue, text centered, xshift=0.1in, yshift=0.16in] (onehotlabel) {\small One-Hot Vector Inputs};
  
  \matrix (embedding) [bmatrix, above left of = matmul, xshift=-0.7in, yshift=0.6in,ampersand replacement=\&] {
    w_{1,1} \& \cdots \& w_{1,|v|}\\[-1ex]
    \vdots  \& \ddots \& \vdots\\
    w_{d,1} \& \cdots \& w_{d,|v|}\\};
  \node [above of=embedding, yshift=0.1in] (embeddinglabel) {\small Embedding Matrix};
  
  % Draw arrows
  \draw [->,thick,line width=0.4mm] (embedding) -- (matmul);
  \draw [->,thick,line width=0.4mm] (oneHotFirst) -- (matmul);
  \draw [->,thick,line width=0.4mm] (matmul) -- (lstmbox);
  \draw [->,thick,line width=0.4mm] (lstmbox) -- (feedforward);
  \draw [->,thick,line width=0.4mm] (lstmbox) to[out=30, in=150, looseness=3] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
  \draw [->,thick,line width=0.4mm] (feedforward.north east) -- (softmax);
  \draw [->,thick,line width=0.4mm] (feedforward) -- (softmax);
  \draw [->,thick,line width=0.4mm] (feedforward.south east) -- (softmax);
  \draw [->,thick,line width=0.4mm,postaction={decorate}] (softmax) -- node[below=1pt] {$|v|$} ++(0.9,0);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotFirst.south) to[bend left] (oneHotSecond.south);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotSecond.south) to[bend left] ++(-0.4,0);
  \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (oneHotLast.south) to [bend right] ++(0.4,0);
  \end{tikzpicture}
  \caption{Trump Character RNN Training Architecture}
\end{figure}

\subsubsection{Embedding Matrix}

\subsubsection{Embedding Matrix}

% It would be good if you could add stuff about why we stack LSTM and the theory behind that.


\subsubsection{Feed-Forward Network}

% loss function
\subsection{Loss Function}

% I haven't mentioned character embedding here

Our network trainer has two inputs, $X \in (V^L)^n$ and $y \in V^n$. $X$ is an $n \times L$ matrix representation of our sequences, where $n$ is the number of sequences and $L$ is the number of characters in each sequence;] $y$ is a vector of $n$ target values, meaning that $y_i$ is the character following sequence $X_i$. Let $p$ be an $n \times |V|$ matrix where each row is a probability distribution over $V : p_i(y_i) = 1$.

Our loss per example is the cross-entropy between the distribution given by the network and the true distribution, which is one-hot. Formally, we say that loss on example $i = L_i = H(p_i, \mathcal{C}(X_i))$. Our total loss on inputs $X$ and $y$ is then $\sum_{i=1}^{n} L_i$.


% our training process
\section{Text Generation Architecture}

\begin{figure}
  \centering
  \tikzset{
    sigmoid/.style={path picture= {
        \begin{scope}[x=1pt,y=10pt]
          \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
        \end{scope}
      }
    },
    every neuron/.style={
      circle,
      draw,
      fill=white,
      minimum size=0.75cm,
    },
    neuron output/.style={
      every neuron,
      sigmoid,
    },
    matrix multiply/.style={
      every neuron,
      font={\Large $\times$}
    },
    every netbox/.style={
      rectangle,
      draw,
      text centered,
      fill=white,
      text width = 1.3cm,
      minimum width = 1.5cm,
      minimum height = 1.5cm,
      rounded corners,
    },
    every left delimiter/.style={xshift=1ex},
    every right delimiter/.style={xshift=-1ex},
    bmatrix/.style={matrix of math nodes,
      inner sep=0pt,
      left delimiter={[},
      right delimiter={]},
      nodes={anchor=center, inner sep=.3333em},
    },
    decoration={
      markings,
      mark= at position 0.5 with {\node {/};}
    }
  }
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth] 
    % Must define nodes in the list before they can be used.
    \node [every netbox, drop shadow] (output) {\small Decision Engine};
    \node [neuron output, left of=output, xshift=-0.35in,  drop shadow] (softmax) {};
    \node [every netbox, left of=softmax, xshift=-0.35in, drop shadow] (feedforward) {\small Feed Forward Network};
    \node [every netbox, left of=feedforward, xshift=-0.45in, drop shadow] (lstmbox) {\small LSTM};
    \node [every neuron, left of=lstmbox, xshift=-0.25in, drop shadow] (matmul) {\Large$\times$};
    \matrix (oneHotFirst) [bmatrix, below left of = matmul, xshift=-0.2in, yshift = -0.5in] {\tiny
      0 \\
      1 \\
      0 \\[-1.5ex]
      \vdots \\
      0\\};
    \matrix (oneHotSecond) [bmatrix, left of = oneHotFirst, xshift=0.1in] {\tiny
      0 \\
      1 \\
      0 \\[-1.5ex]
      \vdots \\
      0\\};
    \node [left of=oneHotSecond, xshift=0.1in] (oneHotContinue) {\Large $\cdots$};
    \matrix (oneHotLast) [bmatrix,left of = oneHotContinue, xshift=0.05in] {\tiny
      1 \\
      0 \\
      0 \\[-1.5ex]
      \vdots \\
      0\\};
    \node [above of=oneHotContinue, text centered, xshift=0.1in, yshift=0.16in] (onehotlabel) {\small One-Hot Vector Inputs};
    
    \matrix (embedding) [bmatrix, above left of = matmul, xshift=-0.7in, yshift=0.6in,ampersand replacement=\&] {
      w_{1,1} \& \cdots \& w_{1,|v|}\\[-1ex]
      \vdots \& \ddots \& \vdots\\
      w_{d,1} \& \cdots \& w_{d,|v|}\\};
    \node [above of=embedding, yshift=0.1in] (embeddinglabel) {\small Embedding Matrix};
    
    % Draw arrows
    \draw [->,thick,line width=0.4mm] (embedding) -- (matmul);
    \draw [->,thick,line width=0.4mm] (oneHotFirst) -- (matmul);
    \draw [->,thick,line width=0.4mm] (matmul) -- (lstmbox);
    \draw [->,thick,line width=0.4mm] (lstmbox) -- (feedforward);
    \draw [->,thick,line width=0.4mm] (lstmbox) to[out=30, in=150, looseness=3] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
    \draw [->,thick,line width=0.4mm] (feedforward.north east) -- (softmax);
    \draw [->,thick,line width=0.4mm] (feedforward) -- (softmax);
    \draw [->,thick,line width=0.4mm] (feedforward.south east) -- (softmax);
    \draw [->,thick,line width=0.4mm,postaction={decorate}] (softmax) -- node[below=1pt] {$|v|$} (output);
    \draw [->,line width=0.4mm] (output) -- ++(0.9,0);
    \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (output.east) -| ++(0.15,0) -| ++(0,-2.5) -| (oneHotLast.south);
    \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotFirst.south) to[bend left] (oneHotSecond.south);
    \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotSecond.south) to[bend left] ++(-0.4,0);
    \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (oneHotLast.south) to [bend right] ++(0.4,0);
  \end{tikzpicture}
  \caption{Trump Character RNN Text Generation Architecture}
\end{figure}

% text generation
\section{Decision Engine}

\subsection{Greedy Sampling}

\subsection{Random First, Greedy Finish}

\subsection{Top-K First, Greedy Finish}

\subsection{Randomization through LSTM Dropout}

\section{Conclusions}

\subsection{Future Work}

\bibliographystyle{ieeetr}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{bib/references}

\end{document}
