\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
 \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tikz}
\usetikzlibrary{matrix, positioning, calc, shadows, decorations.markings, arrows.meta} % decorations.markings is used for the bus symbol on the arrow.

\usepackage{mathtools}  % Used for the paired delimiters
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%

\title{Make Deep Learning Great Again: \\ Character-Level RNN Speech Generation \\ in the Style of Donald Trump}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Benjamin Sherman \\
  Department of Computer Science\\
  University of California, Santa Cruz\\
  Santa Cruz, CA 95064 \\
  \texttt{bcsherma@ucsc.edu} \\
  \And
  Zayd Hammoudeh \\
  Department of Computer Science\\
  University of California, Santa Cruz\\
  Santa Cruz, CA 95064 \\
  \texttt{zayd@ucsc.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  A character-level recurrent neural network (RNN) is a statistical language model capable of producing text that superficially resembles a training corpus. The efficacy of these networks in mimicking Shakespeare, \texttt{Linux} source code, and other forms of text have already been demonstrated. In this paper, we show that character level RNNs are capable of very believably mimicking the language of President Donald J. Trump after training on a corpus of speech transcriptions. We believe our most significant contributions to the study of character level statistical language models are in sampling methodologies; specifically we propose that introducing dropout during the text-generation phase introduces randomness that leads to more believable text.
\end{abstract}

% introduction
\section{Introduction}


% character level RNN overview
\section{Character Level RNNs}

A character level RNN is a statistical model of language. It is statistical in the sense that it views the behavior of language probabilistically. To better understand this, allow that my brain uses a statistical language model and that my job is to try to finish all of your sentences. You say ``I am going to the grocery-''. I would guess with high probability that you are about to say ``store'', though I have to accept that there is a non-zero chance you will say ``outlet''- or, perhaps you will stub your toe and say ``ouch!'' in the middle of your sentence. What a character level RNN does is really no different, except the inferences it learns to make happen on the character level.

Let $\mathcal{C} : V^L \rightarrow P$ be a character-level recurrent neural network, such that $V$ is a vocabulary of approximately 100 characters, $L$ is an arbitrary sequence length, and $P$ is the set of all probability distributions over $V$. More plainly, we can think of a character-level RNN as a function that takes a sequence of $L$~characters and gives us a probability distribution; in particular, it gives the probability of the next character given the previous $L$ characters. As an example of how the network will ideally behave, imagine that you let $p = \mathcal{C}(\texttt{(M,a,k,e, ,A,m,e,r,i,c)})$. If the network is well trained you should find that $p$ predicts \texttt{a} as the next character with high probability.

In order to generate meaningful text with a character level RNN you need to give it a seed, similar to the sentence-to-be-finished from the game described earlier. Prompted with the sequence ``We will build a g'', the RNN will produce a distribution over the vocabulary. We can sample from this distribution and add the resulting character to the sequence. If the sampled character was `r`, as we would hope, then we now have the sequence ``We will build a gr''. We can continue this process for arbitrarily many steps. It is important to note that we can not make networks that accept arbitrarily long sequences as input, so at some point we have to start removing a character from the beginning of the sequence for every character we add to the end of the sequence.

\subsection{Impracticality of Word-Level RNNs}

It is obvious that generating paragraphs of text one character at a time may yield suboptimal results.  As part of the feedback to our initial project proposal, the grader specifically asked why we chose to do a character-level RNN instead of a word-level RNN.  Superficially, a word-level RNN superficially has clear advantages including that it is less likely to produce spelling mistakes and also makes decisions at a coarser granularity which may yield superior results.  When upon a more detail analysis, it is clear that word-level RNNs are impractical.

First, the current Oxford English dictionary has over 170,000 words. A world-level learner would need a separate output node for each of these words.  Such a large output is likely to suffer from underflow and floating point errors that would severely degrade the quality of its predictions.  In addition, training such a large network would be prohibitively long and would extend significantly past the short duration for this project.  Even Google with its near limitless computation and human resources does not do word-level prediction and instead uses morphemes.

One team in the CMPS242 class chose to use a word-level learner.  To address the output-layer size issue mentioned previously, this team reduced the vocabulary to several hundred words.  They also only generated phrases of approximately 10~words.  Such extreme constraints yield results that significantly underachieve character-level RNNs.


% our training process
\section{Training}

% dataset overview
\subsection{Dataset}

% add some statistics and an overview of where we got the data etc.
Character-level RNNs can be provided any user-supplied text as a seed.  While some words are substantially more common than others, it does not change the fact that a character-level RNN must be able to generate meaningful outputs from countless many input seeds. Therefore, to achieve acceptable performance, the training set needs to be especially large.

Although President Trump is credited with being the lead author of over a dozen books~\cite{trumpArtOfTheDeal,trumpSurvivingAtTheTop,trumpArtOfTheComeback,trumpTheAmericaWeDeserve,trumpHowToGetRich,trumpCrippledAmerica,trumpTimeToGetTough,trumpThinkBig,trump101,trumpNeverGiveUp,trumpWhyWeWantYou,trumpBestGolfAdvice,trumpMidasTouch}, we also deliberately chose to exclude them from the training set also for two primary reasons.  First, many of the books featured co-authors or were entirely ghostwritten~\cite{mayerNewYorkerGhostwriter}.  As such, it would be difficult to distinguish Trump's own style from those of his writing partners.  In addition, most of Trump's books date back to the late 1980's through the early 2000's.  Most of the students in the course had not even been born by the time some of these books were written. Hence, the generated text they may yield may not be meaningful to the class's relatively young audience.

Another possible source of training content are Trump's tweets, but we did not use them in this project for multiple reasons.  First, Twitter limits tweets to only 144~characters. As such, tweeters deliberately prune content and commentary to fit within this strict type limit.  This leads to extensive use of abbreviations, skipping of words, or hastags (e.g., ``\#MAGA'' for ``Make America Great Again'') many of which are exclusive to the Twitter platform. For example, one of President Trump's signature Twitter mannerisms is to end a tweet with ``Sad!''; however, this is not done in everyday speech even by the president himself. In addition, similar to at least some of Mr. Trump's books, many of his tweets are ghostwritten.  It has been reported that at least White House social media director Dan Scavino Jr.~\cite{ohlheiser2017} and Trump lawyer, John Dowd~\cite{phillipsBlake2017}, have authored tweets in Trump's name.  These ``imposter'' tweets risk polluting the data set with non-Trump content.

Given the deficiencies associated with training on tweets or the president's book, we decided to exclusively limit ourselves to public speeches made by Mr. Trump since he announced his bid to run for president on June 16, 2015.  Some of these public speeches had already been compiled in repository in~\cite{ryanMcdermottTrumpSpeeches}.  Another repository of largely different speeches had been collected in a separate repository~\cite{pedramNavidTrumpSpeeches}.  Unlike the first set of speeches which was a static collection in text format, the second set used a web scraper to pull tweets from the University of California, Santa Barbara's campaign speech archive~\cite{americanPresidencyProject}.  However, the web scraper had significant bugs that was corrupting the speech output.  We modified the script using Python's \texttt{BeautifulSoup4} package to properly extract the tweets.  We then manually merged the two training sets verifying there were no duplicates.

In total, the training set consisted of more than 115 speeches of varying length.  There were more than 365,000~words and two million training sequences.  We believe the training set size is more than sufficient for a project of this scope, and we are further confident this is shown in the quality of the generated output.

\subsection{Vocabulary}

As with all character-level RNNs, the vocabulary is set of individual characters that may be received as part of an input sequence or generated as part of the output.  Specifically, the vocabulary consists of all letters -- both capitalized and lowercase, digits~(0-9), and punctuation (e.g.,~comma, space, newline, exclamation point, etc.).

For all practical purposes, the set of characters,~$v$, that make up the vocabulary is dictated by the training data set.  In this project, the size of the vocabulary,~$|v|$,

% architecture
\subsection{Learner Architecture}

\begin{figure}
  \centering
  \tikzset{
    sigmoid/.style={path picture= {
        \begin{scope}[x=1pt,y=10pt]
          \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
        \end{scope}
      }
    },
    every neuron/.style={
      circle,
      draw,
      fill=white,
      minimum size=0.75cm,
    },
    neuron output/.style={
      every neuron,
      sigmoid,
    },
    matrix multiply/.style={
      every neuron,
      font={\Large $\times$}
    },
    every netbox/.style={
      rectangle,
      draw,
      text centered,
      fill=white,
      text width = 1.3cm,
      minimum width = 1.5cm,
      minimum height = 1.5cm,
      rounded corners,
    },
    every left delimiter/.style={xshift=1ex},
    every right delimiter/.style={xshift=-1ex},
    bmatrix/.style={matrix of math nodes,
      inner sep=0pt,
      left delimiter={[},
      right delimiter={]},
      nodes={anchor=center, inner sep=.3333em},
    },
    decoration={
      markings,
      mark= at position 0.5 with {\node {/};}
    }
  }
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
  % Must define nodes in the list before they can be used.
  \node [neuron output, drop shadow] (softmax) {};
  \node [every netbox, left of=softmax, xshift=-0.35in, drop shadow] (feedforward) {\small Feed Forward Network};
  \node [every netbox, left of=feedforward, xshift=-0.45in, drop shadow] (lstmbox) {\small LSTM};
  \node [every neuron, left of=lstmbox, xshift=-0.25in, drop shadow] (matmul) {\Large$\times$};
  \matrix (oneHotFirst) [bmatrix, below left of = matmul, xshift=-0.2in, yshift = -0.5in] {\scriptstyle
    \scriptstyle0 \\
    \scriptstyle1 \\
    \scriptstyle0 \\[-1.5ex]
    \scriptstyle\vdots \\
    \scriptstyle0\\};
  \matrix (oneHotSecond) [bmatrix, left of = oneHotFirst, xshift=0.1in] {\scriptstyle
    \scriptstyle0 \\
    \scriptstyle1 \\
    \scriptstyle0 \\[-1.5ex]
    \scriptstyle\vdots \\
    \scriptstyle0\\};
  \node [left of=oneHotSecond, xshift=0.1in] (oneHotContinue) {\Large $\cdots$};
  \matrix (oneHotLast) [bmatrix,left of = oneHotContinue, xshift=0.05in] {\scriptstyle
    \scriptstyle1 \\
    \scriptstyle0 \\
    \scriptstyle0 \\[-1.5ex]
    \scriptstyle\vdots \\
    \scriptstyle0\\};
  \node [above of=oneHotContinue, text centered, xshift=0.1in, yshift=0.16in] (onehotlabel) {\small One-Hot Vector Inputs};

  \matrix (embedding) [bmatrix, above left of = matmul, xshift=-0.7in, yshift=0.6in,ampersand replacement=\&] {
    w_{1,1} \& \cdots \& w_{1,|v|}\\[-1ex]
    \vdots  \& \ddots \& \vdots\\
    w_{d,1} \& \cdots \& w_{d,|v|}\\};
  \node [above of=embedding, yshift=0.1in] (embeddinglabel) {\small Embedding Matrix};

  % Draw arrows
  \draw [->,thick,line width=0.4mm] (embedding) -- (matmul);
  \draw [->,thick,line width=0.4mm] (oneHotFirst) -- (matmul);
  \draw [->,thick,line width=0.4mm] (matmul) -- (lstmbox);
  \draw [->,thick,line width=0.4mm] (lstmbox) -- (feedforward);
  \draw [->,thick,line width=0.4mm] (lstmbox) to[out=30, in=150, looseness=3] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
  \draw [->,thick,line width=0.4mm] (feedforward.north east) -- (softmax);
  \draw [->,thick,line width=0.4mm] (feedforward) -- (softmax);
  \draw [->,thick,line width=0.4mm] (feedforward.south east) -- (softmax);
  \draw [->,thick,line width=0.4mm,postaction={decorate}] (softmax) -- node[below=1pt] {$|v|$} ++(0.9,0);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotFirst.south) to[bend left] (oneHotSecond.south);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotSecond.south) to[bend left] ++(-0.4,0);
  \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (oneHotLast.south) to [bend right] ++(0.4,0);
  \end{tikzpicture}
  \caption{Trump Character RNN Training Architecture}
\end{figure}

\subsubsection{One-Hot Vector Input}

\subsubsection{Embedding Matrix}

\subsubsection{Long Short-Term Memory}

% It would be good if you could add stuff about why we stack LSTM and the theory behind that.


\subsubsection{Feed-Forward Network}


\subsubsection{Softmax Layer}

A softmax layer is function, $\sigma$, that a real-valued vector,~$\mathbf{z}$, of fixed sized,~$\abs{v}$, and returns an equal-sized vector,~$\mathbf{p}$ whose elements are between $0$~and $1$~inclusive.  Hence, $\sigma : \mathbb{R}^{\abs{v}} \rightarrow [0,1]^{\abs{v}}$.  The magnitude of each element $z_k \in \mathbf{z}$, is normalized to create value $p_k \in \mathbf{p}$ using the softmax function where:

\begin{equation}
  p_k = \frac{\exp(z_k)}{\sum_{j=1}^{\abs{v}} \exp(z_j)}\textrm{.}
\end{equation}

Therefore, the softmax layer creates a probability vector as the sum of all its elements is necessarily~$1$. This standardized output is then used by the loss function as described in the next section.

% loss function
\subsection{Loss Function}

% I haven't mentioned character embedding here

Our network trainer has two inputs, $X \in (V^L)^n$ and $y \in V^n$. $X$ is an $n \times L$ matrix representation of our sequences, where $n$ is the number of sequences and $L$ is the number of characters in each sequence;] $y$ is a vector of $n$ target values, meaning that $y_i$ is the character following sequence $X_i$. Let $p$ be an $n \times |V|$ matrix where each row is a probability distribution over $V : p_i(y_i) = 1$.

Our loss per example is the cross-entropy between the distribution given by the network and the true distribution, which is one-hot. Formally, we say that loss on example $i = L_i = H(p_i, \mathcal{C}(X_i))$. Our total loss on inputs $X$ and $y$ is then $\sum_{i=1}^{n} L_i$.


% our training process
\section{Text Generation Architecture}

\begin{figure}
  \centering
  \tikzset{
    sigmoid/.style={path picture= {
        \begin{scope}[x=1pt,y=10pt]
          \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
        \end{scope}
      }
    },
    every neuron/.style={
      circle,
      draw,
      fill=white,
      minimum size=0.75cm,
    },
    neuron output/.style={
      every neuron,
      sigmoid,
    },
    matrix multiply/.style={
      every neuron,
      font={\Large $\times$}
    },
    every netbox/.style={
      rectangle,
      draw,
      text centered,
      fill=white,
      text width = 1.3cm,
      minimum width = 1.5cm,
      minimum height = 1.5cm,
      rounded corners,
    },
    every left delimiter/.style={xshift=1ex},
    every right delimiter/.style={xshift=-1ex},
    bmatrix/.style={matrix of math nodes,
      inner sep=0pt,
      left delimiter={[},
      right delimiter={]},
      nodes={anchor=center, inner sep=.3333em},
    },
    decoration={
      markings,
      mark= at position 0.5 with {\node {/};}
    }
  }
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
    % Must define nodes in the list before they can be used.
    \node [every netbox, drop shadow] (output) {\small Decision Engine};
    \node [neuron output, left of=output, xshift=-0.35in,  drop shadow] (softmax) {};
    \node [every netbox, left of=softmax, xshift=-0.35in, drop shadow] (feedforward) {\small Feed Forward Network};
    \node [every netbox, left of=feedforward, xshift=-0.45in, drop shadow] (lstmbox) {\small LSTM};
    \node [every neuron, left of=lstmbox, xshift=-0.25in, drop shadow] (matmul) {\Large$\times$};
    \matrix (oneHotFirst) [bmatrix, below left of = matmul, xshift=-0.2in, yshift = -0.5in] {\scriptstyle
      \scriptstyle0 \\
      \scriptstyle1 \\
      \scriptstyle0 \\[-1.5ex]
      \scriptstyle\vdots \\
      \scriptstyle0\\};
    \matrix (oneHotSecond) [bmatrix, left of = oneHotFirst, xshift=0.1in] {\scriptstyle
      \scriptstyle0 \\
      \scriptstyle1 \\
      \scriptstyle0 \\[-1.5ex]
      \scriptstyle\vdots \\
      \scriptstyle0\\};
    \node [left of=oneHotSecond, xshift=0.1in] (oneHotContinue) {\Large $\cdots$};
    \matrix (oneHotLast) [bmatrix,left of = oneHotContinue, xshift=0.05in] {\scriptstyle
      \scriptstyle1 \\
      \scriptstyle0 \\
      \scriptstyle0 \\[-1.5ex]
      \scriptstyle\vdots \\
      \scriptstyle0\\};
    \node [above of=oneHotContinue, text centered, xshift=0.1in, yshift=0.16in] (onehotlabel) {\small One-Hot Vector Inputs};

    \matrix (embedding) [bmatrix, above left of = matmul, xshift=-0.7in, yshift=0.6in,ampersand replacement=\&] {
      w_{1,1} \& \cdots \& w_{1,|v|}\\[-1ex]
      \vdots \& \ddots \& \vdots\\
      w_{d,1} \& \cdots \& w_{d,|v|}\\};
    \node [above of=embedding, yshift=0.1in] (embeddinglabel) {\small Embedding Matrix};

    % Draw arrows
    \draw [->,thick,line width=0.4mm] (embedding) -- (matmul);
    \draw [->,thick,line width=0.4mm] (oneHotFirst) -- (matmul);
    \draw [->,thick,line width=0.4mm] (matmul) -- (lstmbox);
    \draw [->,thick,line width=0.4mm] (lstmbox) -- (feedforward);
    \draw [->,thick,line width=0.4mm] (lstmbox) to[out=30, in=150, looseness=3] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
    \draw [->,thick,line width=0.4mm] (feedforward.north east) -- (softmax);
    \draw [->,thick,line width=0.4mm] (feedforward) -- (softmax);
    \draw [->,thick,line width=0.4mm] (feedforward.south east) -- (softmax);
    \draw [->,thick,line width=0.4mm,postaction={decorate}] (softmax) -- node[below=1pt] {$|v|$} (output);
    \draw [->,line width=0.4mm] (output) -- ++(0.9,0);
    \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (output.east) -| ++(0.15,0) -| ++(0,-2.5) -| (oneHotLast.south);
    \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotFirst.south) to[bend left] (oneHotSecond.south);
    \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotSecond.south) to[bend left] ++(-0.4,0);
    \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (oneHotLast.south) to [bend right] ++(0.4,0);
  \end{tikzpicture}
  \caption{Trump Character RNN Text Generation Architecture}
\end{figure}

% text generation
\section{Decision Engine}

We refer to the post-training text generation algorithm as the decision engine. The name is apt because the problem of generation is: given a distribution of `the next character`, how do you decide what the next character is going to be? An obvious answer would be ``take the most likely choice''. What if the distribution you receive is nearly uniform? What if the distribution you receive is not accurate? In this section we will evaluate the merits and drawbacks of greedy, randomized, and mixed decision engines.

\subsection{Greedy Sampling}
The greedy decision engine is in some sense the most obvious: just take the most likely character. A clear advantage is that you are always making the choice that you are always making the most confident choice possible. An issue is that you are not allowing for spontaneity. Text generated with the greedy decision engine tends to enter infinite loops, which we can escape only with the injection of randomness. For example, when we give our best model the seed \textit{``The media is so dishonest.''} and generate with the greedy decision engine, the output is \textit{``They want to stop the people of the world. I want to stop the people of the world. I want to stop the people of the world. I want to stop the...''} etc.

\subsection{Random First, Greedy Finish}
To get out of infinite loops, we need to make our decision engine stochastic. The obvious way to do this would to sample from the distribution returned to us by the network; in other words, the network is giving us a weighted die, so let's roll it. Let's call this the basic random engine.

The problem with the basic random engine is its randomness. The benefit of the greedy engine is that it always makes a confident choice, but the random engine makes no such guarantee. Every time we roll the die given the sequence ``My name is Donald Trum'' there is non-zero chance it will come up \texttt{`Q'}, and this is a huge problem.

One solution is the random-first greedy-finish engine, or RFGF. The basic idea is: always start a new word with a random choice, but in the middle of a word make greedy choices. In practice this means that you use the basic random engine if and only if the last character generated was whitespace, else you use the greedy engine. This gives you the best of both worlds. It is random enough to avoid infinite looping behavior, yet it does not mangle worlds with absurd characters.

\subsection{Top-K First, Greedy Finish}
Observe that with the RFGF engine, it is still possible to make absurd random choices. Because softmax can not assign 0 probability to any character, you might randomly add a space or exlamation mark immediately following whitespace. This illustrates that even though we get a distribution over a large vocabulary, we should assign 0 probability to some things. Our approach is to do a top-k operation before we sample. This means that before you sample from the distribution you throw away everything but the k most likely values. You then renormalize this sub-distribution so that the probabilities sum to 1. Now you have a k-sided that die that you can roll. We call this the top-k first greedy finish engine, or TFGF.

\subsection{Randomization through LSTM Dropout}

\section{Conclusions}

\subsection{Future Work}

\bibliographystyle{ieeetr}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{bib/references}

\end{document}
