\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
 \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}

\usepackage[final]{nips_2017}

% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage[hidelinks]{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{tikz}
\usetikzlibrary{matrix, positioning, calc, shadows, decorations.markings, arrows.meta} % decorations.markings is used for the bus symbol on the arrow.
\usepackage[colorinlistoftodos]{todonotes} % Used for marking up the PDF with notes

\usepackage{mathtools}  % Used for the paired delimiters
\DeclarePairedDelimiter\abs{\lvert}{\rvert}%

\newcommand{\zayd}[1]{\color{red} Zayd: }

\title{Make Deep Learning Great Again: \\ Character-Level RNN Speech Generation \\ in the Style of Donald Trump}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Benjamin Sherman \\
  Department of Computer Science\\
  University of California, Santa Cruz\\
  Santa Cruz, CA 95064 \\
  \texttt{bcsherma@ucsc.edu} \\
  \And
  Zayd Hammoudeh \\
  Department of Computer Science\\
  University of California, Santa Cruz\\
  Santa Cruz, CA 95064 \\
  \texttt{zayd@ucsc.edu} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  A character-level recurrent neural network~(RNN) is a statistical language model capable of producing text that superficially resembles a training corpus. The efficacy of these networks in mimicking Shakespeare, \texttt{Linux} source code, and other forms of text have already been demonstrated. In this paper, we show that character-level RNNs are capable of very believably mimicking the language of President Donald J. Trump after training on a corpus of speech transcriptions. We believe our most significant contributions to the study of character level statistical language models are in sampling methodologies; specifically we propose that introducing dropout during the text-generation phase introduces randomness that leads to more believable text.
\end{abstract}

% introduction
\section{Introduction}

The generation of natural language text involves not only the ability to produce speech, but also the ability to understand the relationship between words.~\cite{mitkov2009}  In addition, natural language takes many very different forms including colloquial, formal, legal, mathematical/scientific, etc.  Speech patterns are uniquely individual and are influenced by one's background, motivations, and biases.  

Natural language generation (NLG) is an open area of research drawing in corporate heavyweights such as Microsoft with Cortana, Amazon with its Alexa platforms, Apple via its Siri subsidiary, and Google through Android and its ``Home'' product line.  All of these attempts at NLG have focused on creating products that speak in a generic manner with broad customer appeal.  By itself, that is an immense task but to an extent sterilizes the speech to be as globally non-offensive as possible.

In this project, we simplify the NLG problem by trying to generate speech in the style of only one person -- President Donald J. Trump; our selection of him as our focus is based on a very specific rationale.  First, Mr. Trump is a particularly polarizing figure in politics meaning the vast majority of people in the country are very familiar with his style.  In addition, President Trump has many cliched refrains that are repeated often such ``build the wall,'' ``make America great again,'' ``little rocketman,'' etc. We expect that this repetition should make it easier for a learner to emulate Mr. Trump's speech essence.  Furthermore, the president tends to use ``highly simplistic'' words in a ``grammatically awkward'' fashion.~\cite{goldhill2017}  Therefore, if the speech we generate has any defects in structure -- grammatical or otherwise -- or if uses infantile language, we expect the audience may attribute these shortcomings to the president himself instead of our tool (especially in a place where Mr. Trump is nearly unanimously derided like Santa Cruz).

\cite{karapathy2015}~demonstrated the surpising effectiveness ofcharacter-level recurrent neural at generating both natural language and structured text. His work was done in the Lua programming language.  In this project, we attempt to verify his findings with our Trump character-level RNN written in TensorFlow.  We provide more background on character-level RNNs in the next section.


% character level RNN overview
\subsection{Character-Level RNNs}

A character-level RNN is a statistical model of language in the sense that it views the behavior of language probabilistically. To better understand this, allow that my brain uses a statistical language model and that my job is to try to finish all of your sentences. You say ``I am going to the grocery-''. I would guess with high probability that you are about to say ``store'', though I have to accept that there is a non-zero chance you will say ``outlet''-- or, perhaps you will stub your toe and say ``ouch!'' in the middle of your sentence. What a character-level RNN does is really no different, except that the inferences it learns to make happen on the character level.

Let $\mathcal{C} : v^L \rightarrow \Pi$ be a character-level recurrent neural network, such that $v$ is a vocabulary of approximately 100 characters, $L$ is an arbitrary sequence length, and $\Pi$ is the set of all probability distributions over $v$. More plainly, we can think of a character-level RNN as a function that takes a sequence of $L$~characters and gives us a probability distribution; in particular, it gives the probability of the next character given the previous $L$ characters. As an example of how the network will ideally behave, imagine that you let $p = \mathcal{C}(\texttt{(M,a,k,e, ,A,m,e,r,i,c)})$. If the network is well trained you should find that $p$ predicts ``\texttt{a}'' as the next character with high probability.

In order to generate meaningful text with a character-level RNN, a seed needs to be provided.  This represents the sentence or thought to be finished using the game described earlier. Prompted with the sequence ``We will build a g'', the RNN will produce a distribution over the vocabulary. We then sample from this distribution and add the resulting character to the sequence. If the sampled character was ``\texttt{r}'', as we would hope, then we now have the sequence ``We will build a gr''. We can continue this process for arbitrarily many characters. It is important to note that we can not make networks that accept arbitrarily long sequences as input, so at some point we have to start removing a character from the beginning of the sequence for every character we add to the end of the sequence.

\subsection{Impracticality of Word-Level RNNs}\label{sec:wordLevelRnn}

It is obvious that generating paragraphs of text one character at a time may yield suboptimal results.  As part of the feedback to our initial project proposal, the grader specifically asked why we chose to use a character-level RNN instead of making decisions at the word-level.  Superficially, a word-level RNN has clear advantages including that it would not produce spelling errors and also that it makes decisions at a coarser granularity, which would be expected to yield superior results.  However, upon a more detailed analysis, it is clear that word-level RNNs are impractical.

First, the current Oxford English dictionary has over 170,000 words.~\cite{oxfordWordCount} A world-level learner would need a separate output node for each of these words (there may be optimizations that could be made to reduce this output count such as using morphemes but that is no longer a word-level RNN).  Such a large output is likely to suffer from underflow and floating point errors that would severely degrade the quality of its predictions.  In addition, training such a large network would be prohibitively long and would extend significantly past the short duration of this project.  Even Google with its near limitless computation and human resources does not do word-level prediction.

One team in the CMPS242 class chose to use a word-level learner.  To address the output-layer size issue mentioned previously, this team reduced the vocabulary to several hundred words.  They also only generated phrases of approximately 10~words or less.  Such extreme constraints yield results that significantly underachieve character-level RNNs.

% our training process
\section{Training}\label{sec:training}

This section describes the techniques we employed to train our Trump character-level RNN.  Specifically, it outlines the training dataset, the structure of our base neural network as well as implementation-level details including the optimizer, learning rate, and batch size.

% dataset overview
\subsection{Dataset}

% add some statistics and an overview of where we got the data etc.
Character-level RNNs can be provided any user-supplied text as a seed.  While some words are substantially more common than others, it does not change the fact that a character-level RNN must be able to generate meaningful outputs from countless many input seeds. Therefore, to achieve acceptable performance, the training set needs to be especially large.

Although President Trump is credited with being the lead author of over a dozen books~\cite{trumpArtOfTheDeal,trumpSurvivingAtTheTop,trumpArtOfTheComeback,trumpTheAmericaWeDeserve,trumpHowToGetRich,trumpCrippledAmerica,trumpTimeToGetTough,trumpThinkBig,trump101,trumpNeverGiveUp,trumpWhyWeWantYou,trumpBestGolfAdvice,trumpMidasTouch}, we also deliberately chose to exclude them from the training set also for two primary reasons.  First, many of the books featured co-authors or were entirely ghostwritten~\cite{mayerNewYorkerGhostwriter}.  As such, it would be difficult to distinguish Trump's own style from those of his writing partners.  In addition, most of Trump's books date back to the late 1980's through the early 2000's.  Most of the students in the course had not even been born by the time some of these books were written. Hence, the generated text they may yield may not be meaningful to the class's relatively young audience.

Another possible source of training content are Trump's tweets, but we did not use them in this project for multiple reasons.  First, Twitter limits tweets to only 144~characters. As such, tweeters deliberately prune content and commentary to fit within this strict type limit.  This leads to extensive use of abbreviations, skipping of words, or hastags (e.g.,~``\#MAGA'' for ``Make America Great Again'') many of which are exclusive to the Twitter platform. For example, one of President Trump's signature Twitter mannerisms is to end a tweet with ``Sad!''; however, this is not done in everyday speech even by the president himself. In addition, similar to at least some of Mr. Trump's books, many of his tweets are ghostwritten.  It has been reported that at least White House social media director Dan Scavino Jr.~\cite{ohlheiser2017} and Trump lawyer, John Dowd~\cite{phillipsBlake2017}, have authored tweets in Trump's name.  These ``imposter'' tweets risk polluting the data set with non-Trump content.

Given the deficiencies associated with training on tweets or the president's book, we decided to exclusively limit ourselves to public speeches made by Mr. Trump since he announced his bid to run for president on June 16, 2015.  Some of these public speeches had already been compiled in repository in~\cite{ryanMcdermottTrumpSpeeches}.  Another repository of largely different speeches had been collected in a separate repository~\cite{pedramNavidTrumpSpeeches}.  Unlike the first set of speeches which was a static collection in text format, the second set used a web scraper to pull tweets from the University of California, Santa Barbara's campaign speech archive~\cite{americanPresidencyProject}.  However, the web scraper had significant bugs that was corrupting the speech output.  We modified the script using Python's \texttt{BeautifulSoup4} package to properly extract the tweets.  We then manually merged the two training sets verifying there were no duplicates.

In total, the training set consisted of more than 115 speeches of varying length.  There were more than 365,000~words and two million training sequences.  We believe the training set size is more than sufficient for a project of this scope, and we are further confident this is shown in the quality of the generated output.

\subsection{Vocabulary}

As with all character-level RNNs, the vocabulary is set of individual characters that may be received as part of an input sequence or generated as part of the output.  Specifically, the vocabulary consists of all letters -- both capitalized and lowercase, digits~(0-9), and punctuation (e.g.,~comma, space, newline, exclamation point, etc.).

The set of characters,~$v$, that comprise the vocabulary is dictated by the training data set.  In this project, the size of the vocabulary,~$|v|$, was~97.

% architecture
\subsection{Learner Architecture}

\begin{figure}
  \centering
  \tikzset{
    sigmoid/.style={path picture= {
        \begin{scope}[x=1pt,y=10pt]
          \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
        \end{scope}
      }
    },
    every neuron/.style={
      circle,
      draw,
      fill=white,
      minimum size=0.75cm,
    },
    neuron output/.style={
      every neuron,
      sigmoid,
    },
    matrix multiply/.style={
      every neuron,
      font={\Large $\times$}
    },
    every netbox/.style={
      rectangle,
      draw,
      text centered,
      fill=white,
      text width = 1.3cm,
      minimum width = 1.5cm,
      minimum height = 1.5cm,
      rounded corners,
    },
    every left delimiter/.style={xshift=1ex},
    every right delimiter/.style={xshift=-1ex},
    bmatrix/.style={matrix of math nodes,
      inner sep=0pt,
      left delimiter={[},
      right delimiter={]},
      nodes={anchor=center, inner sep=.3333em},
    },
    decoration={
      markings,
      mark= at position 0.5 with {\node {/};}
    }
  }
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
  % Must define nodes in the list before they can be used.
  \node [neuron output, drop shadow] (softmax) {};
  \node [every netbox, left of=softmax, xshift=-0.35in, drop shadow] (feedforward) {\small Feed Forward Network};
  \node [every netbox, left of=feedforward, xshift=-0.45in, drop shadow] (lstmbox) {\small LSTM};
  \node [every neuron, left of=lstmbox, xshift=-0.25in, drop shadow] (matmul) {\Large$\times$};
  \matrix (oneHotFirst) [bmatrix, below left of = matmul, xshift=-0.2in, yshift = -0.5in] {\scriptstyle
    \scriptstyle0 \\
    \scriptstyle1 \\
    \scriptstyle0 \\[-1.5ex]
    \scriptstyle\vdots \\
    \scriptstyle0\\};
  \matrix (oneHotSecond) [bmatrix, left of = oneHotFirst, xshift=0.1in] {\scriptstyle
    \scriptstyle0 \\
    \scriptstyle1 \\
    \scriptstyle0 \\[-1.5ex]
    \scriptstyle\vdots \\
    \scriptstyle0\\};
  \node [left of=oneHotSecond, xshift=0.1in] (oneHotContinue) {\Large $\cdots$};
  \matrix (oneHotLast) [bmatrix,left of = oneHotContinue, xshift=0.05in] {\scriptstyle
    \scriptstyle1 \\
    \scriptstyle0 \\
    \scriptstyle0 \\[-1.5ex]
    \scriptstyle\vdots \\
    \scriptstyle0\\};
  \node [above of=oneHotContinue, text centered, xshift=0.1in, yshift=0.1in] (onehotlabel) {\small One-Hot Vector Inputs};

  \matrix (embedding) [bmatrix, above left of = matmul, xshift=-0.7in, yshift=0.6in,ampersand replacement=\&] {
    w_{1,1} \& \cdots \& w_{1,|v|}\\[-1ex]
    \vdots  \& \ddots \& \vdots\\
    w_{d,1} \& \cdots \& w_{d,|v|}\\};
  \node [above of=embedding, yshift=0.01in] (embeddinglabel) {\small Embedding Matrix};

  % Draw arrows
  \draw [->,thick,line width=0.4mm] (embedding) -- (matmul);
  \draw [->,thick,line width=0.4mm] (oneHotFirst) -- (matmul);
  \draw [->,thick,line width=0.4mm] (matmul) -- (lstmbox);
  \draw [->,thick,line width=0.4mm] (lstmbox) -- (feedforward);
  \draw [->,thick,line width=0.4mm] (lstmbox) to[out=30, in=150, looseness=3] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
  \draw [->,thick,line width=0.4mm] (feedforward.north east) -- (softmax);
  \draw [->,thick,line width=0.4mm] (feedforward) -- (softmax);
  \draw [->,thick,line width=0.4mm] (feedforward.south east) -- (softmax);
  \draw [->,thick,line width=0.4mm,postaction={decorate}] (softmax) -- node[below=1pt] {$|v|$} ++(0.9,0);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotFirst.south) to ++(0, -0.4);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotSecond.south) to ++(0, -0.4);
  \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotLast.south) to ++(0, -0.4);
  \node [below of=oneHotContinue, text centered, xshift=0.1in, yshift=-0.325in] (seedTextInput) {\small Training Input};
  \end{tikzpicture}
  \caption{Trump Character-Level RNN Training Architecture}\label{fig:trainingRnnArchitecture}
\end{figure}

The base neural network architecture we used is similar to the one proposed in homework~\#5 and is shown in Figure~\ref{fig:trainingRnnArchitecture}; we specifically refer to the training architecture as the ``base'' since it is a subset of our complete architecture which is described in Section~\ref{sec:generationRnnArchitecture}.  

The training architecture consists of five primary components, namely: the one-hot vector input, embedding matrix, long-short term memory (LSTM) RNN, the feed-forward network, and finally the softmax-output layer.

\subsubsection{One-Hot Vector Input}

As the name indicates, a ``one-hot'' vector is a zero vector with exactly one element in the vector equal to one.  Each element in the vector corresponds to a fixed element in the vocabulary.  Hence, the size of the vector is~$\abs*{v}$, i.e.,~the size of the vocabulary. Each training example consists of an ordered series of one-hot vectors and a single output one-hot vector.  The neural network defines a maximum sequence length which is the maximum number of one-hot vectors that can correspond to a single output.  For our network, this maximum sequence length was~50.

\subsubsection{Embedding Matrix}

The embedding matrix is a learned object that maps the one-hot vectors from size~$\abs*{v}$ to size~$d$, which is the number of hidden layers in the LSTM block (see Section~\ref{sec:lstm}).  An embedding matrix may seem superfluous for this type of project since the size of the vocabulary is comparatively small.  However, we decided to use leave it in our design for several reasons.  First, we knew that the duration of the project was relatively short (about two weeks) and our team was smaller than other (only two members).  Hence, we expected we may need to accelerate the training reducing the number of neurons in the LSTM.  In such a scenario, the embedding matrix would have served as a learned dimensionality-reduction technique.  

In addition, it is common for the number of elements in the training set to vary by between 1-5~elements depending on the specific training dataset used.  Hence, if a non-Trumpian dataset was to be used for training, it would not be required to train the entire network from scratch.  Rather, on the output and embedding matrices would need to be retrained. (Note this feature is not currently supported in our implementation.)


\subsubsection{Long Short-Term Memory}\label{sec:lstm}

Long short-term memory (LSTM) cells are recurrent neural network components, each comprising several carefully arranged neurons and gates. If our recurrent network accepts sequences of length $L$, then the network must contain a sequence of $L$ LSTM cells. The $i^{\text{th}}$ character of the input sequence will feed into the $i^{\text{th}}$ LSTM cell, and the $i^{\text{th}}$ LSTM cell will also feed into the $i+1^{\text{th}}$ LSTM cell.

We have also added support for a multi-LSTM recurrent layer. This means that each LSTM cell from the singular definition is replaced by an LSTM layer comprised of two or more LSTM cells. By doing this, you essentially give the network more hardware to memorize sequences with. The connections between these LSTM layers are full, meaning that every cell of the $i^{\text{th}}$ LSTM layer feeds into every cell of the $i+1^{\text{th}}$ LSTM layer.

\paragraph{Dropout}\label{sec:dropout}

Dropout is a computationally inexpensive technique to provide regularization during neural network training.  It approximates the bagging technique of an ensemble classifier by training different \textit{versions} of the learner through the temporary deletion of units within the network.\cite{goodfellow2016}  TensorFlow supports input, state, and output dropout, which may be used independently or in conjunction with one another.  We experimented with different dropout modes and deletion probabilities.  We did not see a significant difference between the settings and settled on output dropout with a keep probability~0.8.

\subsubsection{Feed-Forward Network}

In the same way that the embedding matrix maps the network's input to the LSTM's input, the feed-forward network maps the output of the LSTM to the output of network (i.e.,~the softmax layer).  Most of the power of a character-level RNN comes from its LSTM; hence, to prevent extreme overfitting, we opted for a simple feed-forward network with only a single hidden layer of 256~fully-connected neurons.   We also observed that we achieved superior results if we used the rectified linear (relu)~activation function for both the hidden and output layers.  We believe this is believe this is because relu allows for greater differentiation between likely characters.


\subsubsection{Softmax Layer}

A softmax layer is a function,~$\sigma$, that takes a real-valued vector,~$\mathbf{x}$, of fixed sized,~$\abs*{v}$, and returns an equal-sized vector,~$\mathbf{p}$ whose elements are between $0$~and $1$~inclusive.  Hence, $\sigma : \mathbb{R}^{\abs*{v}} \rightarrow [0,1]^{\abs*{v}}$. For each element~$k$, the softmax normalizes element's magnitude using the Softmax formula, defined as:

\begin{equation}
  p_k = \frac{\exp(x_k)}{\sum_{j=1}^{\abs*{v}} \exp(x_j)}\textrm{.}
\end{equation}

Therefore, the output vector~$\mathbf{y}$ is necessarily a probability vector, the sum of whose elements adds to one. The softmax function has multiple advantages including that it creates a standardized output and that it enables the use of well studied loss functions, such as cross-entropy which is described in the next section.

% loss function
\subsection{Loss Function}

We use cross-entropy between the true and predicted distributions as our loss. For a sequence $x$~preceding character~$c$, we define the true distribution to be $p: p(c) = 1$. In the same example, the predicted distribution is the output of the network given the sequence, or $\mathcal{C}(x)$. For a particular example, if our sequence is $x$ and the true distribution is $y$ then the loss on this example is the cross entropy between $y$ and $\mathcal{C}(x)$, or $H(y, \mathcal{C}(x))$. We take the total loss on a set of examples to be the sum of losses per example.

\subsection{Batch Size, Learning Rate, and Optimizer}

Three important factors that can have a significant impact on the training of a character-level RNN are batch size, learning rate, and optimizer.  A smaller batch size often leads to a better learner.  As such, we set our batch size to only 50~sequences.  As such, a single training epoch required approximately 40,000 batches. This meant training a single epoch on a modern high-end CPU took about one hour.  With so many batches, selecting a high learning rate would risk later batches wiping out the changes made in early ones.  As such, we set the learning rate very low,~0.0005. 

In homework~\#5, we observed that TensorFlow's \texttt{AdamOptimizer} (which implements adaptive moment estimation~\cite{kingma2014}) converged the fastest and generally produced the best results.  As such, we used it again here for this project.  

\subsection{Variable Sequence Length Training}

Training time and output quality are competing concerns when selecting the sequence length of the learner.  A shorter sequence limits the contextual information the learner can use when predicting an output, but longer sequences increase training time.  Likewise, sequences longer than 200-300 characters are not generally correlated with improved output quality.

In homework~\#5, each tweet had a fixed, predefined sequence length.  In contrast, during speech generation, the learner is provided a user-created seed text that may be shorter or longer than the maximum sequence length,~$m$.  The case of a longer sequence is trivially handled by only considering the $m$~most recent characters.  In contrast, shorter sequences are more challenging since generally the learner is only trained at the maximum sequence length.  To address this, we added \textit{variable sequence length training} where we train our network on multiple sequence lengths between the minimum allowed (e.g.,~10 characters) and~$m$.  This is expected to improve our network's performance on shorter sequences.

% our training process
\section{Text Generation}

To enable text generation, changes must be made to the training architecture described in Section~\ref{sec:training}.  This section discusses these architectural modifications as well as how we seed the generator.

\subsection{Seeding}

Rather than generating text from random noise, we believes that users have a much better experience if they can supply the initial \textit{seed} text upon which the learner will build the rest of its output.  To ensure that the architecture has sufficient context upon which to begin generating text, we required that the seed string be at least 10~characters long.  Text generation continues until the specified number of characters (e.g.,~500) have been generated.

\subsection{Text Generation Architecture}\label{sec:generationRnnArchitecture}

The neural network in Figure~\ref{fig:trainingRnnArchitecture} is referred to as the ``base architecture'' since it is only a subset of the complete system used during text generation.  Figure~\ref{fig:generationArchitecture} shows the complete text-generation architecture, and there are two primary changes from the previously mentioned base.  First, we add an entirely new block to the network, which we refer to as the ``decision engine.''  Its role is to select a single output character from the softmax probability vector of length~$\abs*{v}$; while this may seem like a trivial task, we explain in more detail the specific challenges of the decision engine in Section~\ref{sec:decisionEngine}. 

The second change is that each generated output character is fed back into the network to generate the next character.  Similarly, the previous sequence of characters is shifted by one with the least recent character removed if the sequence length is longer than the maximum,~$m$ (e.g.,~50).  This approach allows the architecture to generate sequences of characters that could theoretically be infinitely long.

\begin{figure}
  \centering
  \tikzset{
    sigmoid/.style={path picture= {
        \begin{scope}[x=1pt,y=10pt]
          \draw plot[domain=-6:6] (\x,{1/(1 + exp(-\x))-0.5});
        \end{scope}
      }
    },
    every neuron/.style={
      circle,
      draw,
      fill=white,
      minimum size=0.75cm,
    },
    neuron output/.style={
      every neuron,
      sigmoid,
    },
    matrix multiply/.style={
      every neuron,
      font={\Large $\times$}
    },
    every netbox/.style={
      rectangle,
      draw,
      text centered,
      fill=white,
      text width = 1.3cm,
      minimum width = 1.5cm,
      minimum height = 1.5cm,
      rounded corners,
    },
    every left delimiter/.style={xshift=1ex},
    every right delimiter/.style={xshift=-1ex},
    bmatrix/.style={matrix of math nodes,
      inner sep=0pt,
      left delimiter={[},
      right delimiter={]},
      nodes={anchor=center, inner sep=.3333em},
    },
    decoration={
      markings,
      mark= at position 0.5 with {\node {/};}
    }
  }
  \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
    % Must define nodes in the list before they can be used.
    \node [every netbox, drop shadow] (output) {\small Decision Engine};
    \node [neuron output, left of=output, xshift=-0.35in,  drop shadow] (softmax) {};
    \node [every netbox, left of=softmax, xshift=-0.35in, drop shadow] (feedforward) {\small Feed Forward Network};
    \node [every netbox, left of=feedforward, xshift=-0.45in, drop shadow] (lstmbox) {\small LSTM};
    \node [every neuron, left of=lstmbox, xshift=-0.25in, drop shadow] (matmul) {\Large$\times$};
    \matrix (oneHotFirst) [bmatrix, below left of = matmul, xshift=-0.2in, yshift = -0.5in] {\scriptstyle
      \scriptstyle0 \\
      \scriptstyle1 \\
      \scriptstyle0 \\[-1.5ex]
      \scriptstyle\vdots \\
      \scriptstyle0\\};
    \matrix (oneHotSecond) [bmatrix, left of = oneHotFirst, xshift=0.1in] {\scriptstyle
      \scriptstyle0 \\
      \scriptstyle1 \\
      \scriptstyle0 \\[-1.5ex]
      \scriptstyle\vdots \\
      \scriptstyle0\\};
    \node [left of=oneHotSecond, xshift=0.1in] (oneHotContinue) {\Large $\cdots$};
    \matrix (oneHotLast) [bmatrix,left of = oneHotContinue, xshift=0.05in] {\scriptstyle
      \scriptstyle1 \\
      \scriptstyle0 \\
      \scriptstyle0 \\[-1.5ex]
      \scriptstyle\vdots \\
      \scriptstyle0\\};
    \node [above of=oneHotContinue, text centered, xshift=0.1in, yshift=0.1in] (onehotlabel) {\small One-Hot Vector Inputs};

    \matrix (embedding) [bmatrix, above left of = matmul, xshift=-0.7in, yshift=0.6in,ampersand replacement=\&] {
      w_{1,1} \& \cdots \& w_{1,|v|}\\[-1ex]
      \vdots  \& \ddots \& \vdots\\
      w_{d,1} \& \cdots \& w_{d,|v|}\\};
    \node [above of=embedding, yshift=0.01in] (embeddinglabel) {\small Embedding Matrix};

    % Draw arrows
    \draw [->,thick,line width=0.4mm] (embedding) -- (matmul);
    \draw [->,thick,line width=0.4mm] (oneHotFirst) -- (matmul);
    \draw [->,thick,line width=0.4mm] (matmul) -- (lstmbox);
    \draw [->,thick,line width=0.4mm] (lstmbox) -- (feedforward);
    \draw [->,thick,line width=0.4mm] (lstmbox) to[out=30, in=150, looseness=3] (lstmbox); % Numbers represent location on the unit circle.  0 is due east, 90 due north, 180 due west, and 270 due south.
    \draw [->,thick,line width=0.4mm] (feedforward.north east) -- (softmax);
    \draw [->,thick,line width=0.4mm] (feedforward) -- (softmax);
    \draw [->,thick,line width=0.4mm] (feedforward.south east) -- (softmax);
    \draw [->,thick,line width=0.4mm,postaction={decorate}] (softmax) -- node[below=1pt] {$|v|$} (output);
    \draw [->,line width=0.4mm] (output) -- ++(0.9,0);
    \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (output.east) -| ++(0.15,0) -| ++(0,-2.5) -| (oneHotLast.south);
    \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotFirst.south) to[bend left] (oneHotSecond.south);
    \draw [{Latex[scale=0.8]}-,line width=0.4mm,dashed] (oneHotSecond.south) to[bend left] ++(-0.4,0);
    \draw [-{Latex[scale=0.8]},line width=0.4mm,dashed] (oneHotLast.south) to [bend right] ++(0.4,0);
  \end{tikzpicture}
  \caption{Trump Character-Level RNN Text Generation Architecture}\label{fig:generationArchitecture}
\end{figure}

% text generation
\section{Decision Engine}\label{sec:decisionEngine}

We refer to the character selection algorithm as the \textit{decision engine}. The name is apt because the problem of generation is: given a distribution for the ``next character,'' how do you decide what the next character is going to be? An obvious answer would be ``take the most likely choice.'' What if the distribution you receive is nearly uniform? What if the distribution you receive is not accurate? In this section, we evaluate the advantages and disadvantages of greedy, randomized, and mixed decision engines.

\subsection{Greedy Sampling}\label{sec:greedySampling}

The greedy decision engine is in some sense the most obvious: just take the most likely character. A clear advantage is that the most confident choice is always made. This necessarily prevents any degree of spontaneity. As such, text generated with the greedy decision engine tends to enter infinite loops. For example, when we give our best model the seed \textit{``The media is so dishonest.''} and generate text with the greedy decision engine, the output is \textit{``They want to stop the people of the world. I want to stop the people of the world. I want to stop the people of the world. I want to stop the...''} etc.

\subsection{Random First, Greedy Finish (RFGF)}

To prevent infinite loops, we make the network stochastic. The obvious way to do this would to be to perform a weighted sample of the output of the softmax layer; in other words, the network inherently provides a weighted dice that we can roll to select the output character.

The problem with this scheme is the unconstrained randomness. The benefit of the greedy engine is that it always makes a confident choice, but the random engine provides no such guarantee. Every time we roll the die given the sequence ``My name is Donald Trum,'' there is non-zero chance it will come up ``\texttt{Q}'', and this is a very serious problem.

One solution is the random-first greedy-finish~(RFGF) engine. The basic idea is: always start a new word with a random choice, but in the middle of a word make greedy choices. In practice, this means that the basic random engine is used if and only if the preceding character was whitespace; otherwise, the standard greedy engine is used. This gives provides an adequate balance of the concerns. It is random enough to avoid the infinite looping behavior, yet it does not mangle words with absurd characters.

\subsection{Top-$\mathbf{\textit{k}}$ First, Greedy Finish (TFGF)}

Since the softmax function does not assign zero probability to any character, there is still a chance that absurd letter choices will be made including putting an exclamation point after a whitespace or inserting a newline mid-sentence.  These simple examples illustrate that when selecting characters at random, the learner must assign zero probability to some characters. We achieve this by having the learner consider only the top-$k$ characters with the highest probability (we used $k=5$ in our experiments). The resulting sub-distribution is then renormalized so that the probabilities sum to one. Now you have a $k$-sided that die that you can roll. We call this the top-$k$ first greedy-finish~(TFGF) engine.


\subsection{Randomization through LSTM Dropout}

As explained in Section~\ref{sec:dropout}, dropout is traditionally a technique used exclusively during training.  It is not generally used in a ``live'' environment producing real outputs.  However, dropout is very computationally inexpensive and has been highly optimized in TensorFlow making it especially fast.  Likewise, using dropout during text generation eliminates the need for random guessing that is associated with both RFGF and TFGF.

We have observed that the best form of randomization may in fact be dropout \textit{during generation}.  We are currently in the process of implementing two new decision engine approaches, namely Dropout First, Greedy Finish, which enables dropout only for the first character after a whitespace as well as Dropout And Greedy Always~(DAGA) where dropout is permanently enabled and the greedy choice is always made.  Preliminary data indicates that the latter approach has execution time, which is very close to that of the standard greedy sampling described in Section~\ref{sec:greedySampling}.

\section{Conclusions}

As demonstrated in class, we successfully implemented a character-level RNN that produces text in the style of Donald J. Trump.  The generated output was so realistic that many in the class could not distinguish it from the real thing.  It is important to note that not all text produced by our system is coherent; the classroom example we presented was specifically selected since it was significantly superior to the typical output.  It is not uncommon that the generator produces incoherent sentences, which means that possible improvements to our system remain.  The next section describes a new approach to character-level RNNs that we believe has the potential to significantly improve output quality.

\subsection{Future Work}

Our RFGF and TFGF engines provide clear advantages over a straight greedy strategy.  However, at their core, RFGF and TFGF are both still greedy.  They make point in time decisions about the best character to select using exclusively past information, and once a decision is made, it cannot be undone.  Rather than selecting a character based solely on past data, we expect a far better decision could be made if we consider the effect of the current decision on \textit{future} decisions as well.

In Section~\ref{sec:wordLevelRnn}, we explained that word-level RNNs are impractical for a multitude of reasons. However, we believe that through character-level decisions it may be possible to achieve near word-level results.  For example, at the start of a word, rather than immediately selecting a character, the system could select the top-$k$ characters and complete the resulting $k$~words using our greedy-finish approach.  The resulting $k$ words could be examined and the best one selected.  This approach reduces the number of possible outputs that must be considered at any given time from more than 170,000~words to just~$k$.  Likewise, rather than making word-level decisions, even better results still may be achieve if n-gram decisions were made.  For example, the $k$~best words could be constructed at a given point in time.  From there, each of their $k$-best descendants could also be constructed.  This process would be repeated until a phrase of length $n$ is built.  

One of the primary challenges of this word or n-gram-based approach is quantitatively deciding the ``best'' selection.  Coherent speech is subjective, and given the simplistic and awkward nature of Trump's speech~\cite{goldhill2017}, we expect that many objective metrics may perform poorly. One approach proposed by Manfred was to select the one with the highest probability (normalized by word length). This approach appears plausible but requires further study.

We are still implementing this aspect of our learner.  The implementation is not complete so we are unable to report the results in this document.  However, we expect for this work to be completed in early~2018.

\bibliographystyle{plain}
% argument is your BibTeX string definitions and bibliography database(s)
\bibliography{bib/references}

\end{document}
